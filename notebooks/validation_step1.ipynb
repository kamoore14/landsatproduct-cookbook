{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be0eb869-631f-4f0a-8ddd-b61b2e9c0331",
   "metadata": {},
   "source": [
    "---\n",
    "title: Validating your data product \n",
    "authors: \n",
    "- name: Genevive Clow\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68739192-2d47-4b83-9d94-2c8c4fa447fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install xarray==2024.05.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b989a0-346a-4949-83d3-074be8f1ff9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "#%pip install pykrige\n",
    "#%pip install xarray\n",
    "# %pip install xarray==2024.05.0\n",
    "\n",
    "%matplotlib widget\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d719bcd8-23d0-4ef5-bc15-5e56e83e4f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "#from skimage import exposure\n",
    "#from skimage.io import imsave, imread\n",
    "#from osgeo import ogr\n",
    "import pystac_client\n",
    "from pyproj import Transformer\n",
    "from datetime import date, timedelta, datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import geoviews as gv\n",
    "import hvplot.pandas\n",
    "import intake\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "import intake\n",
    "from pyproj import Proj, transform\n",
    "#from osgeo import gdal\n",
    "from sklearn.neighbors import BallTree\n",
    "import earthaccess\n",
    "import gzip\n",
    "import s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3581d67-e877-45f7-9391-b7ecab5ca31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for progress bar\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "from ipywidgets import interact, Dropdown\n",
    "import time\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "import boto3\n",
    "import rasterio as rio\n",
    "from rasterio.features import rasterize\n",
    "from rasterio.session import AWSSession\n",
    "import dask\n",
    "import os\n",
    "import rioxarray\n",
    "from rasterio.enums import Resampling\n",
    "from rasterio.warp import reproject\n",
    "from rasterio.warp import Resampling as resample\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy\n",
    "#from pykrige.ok import OrdinaryKriging\n",
    "from sklearn.linear_model import LinearRegression, RANSACRegressor\n",
    "from scipy.odr import Model, RealData, ODR\n",
    "import scipy.odr as odr\n",
    "import scipy\n",
    "import statsmodels.formula.api as smf\n",
    "from shapely.geometry.polygon import Polygon, Point\n",
    "import pygmt\n",
    "import gc\n",
    "import pytz\n",
    "import pyproj\n",
    "import math\n",
    "from pathlib import Path\n",
    "from matplotlib.patches import Polygon as Pgon\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import SSTutils as sut\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0eae27-4580-434a-a934-6a5c80821a11",
   "metadata": {},
   "source": [
    "## Why do we need to validate?\n",
    "Now we have our calibrated SST data product! ü•≥ We're almost ready to use this data product for our scientific analysis, but there is still one more important step. As a quick reminder, our data product was derived from top-of-atmosphere radiance measurements. We then converted these radiances into actual SST values (in units of temperature) by calibrating with another satellite (MODIS). But can we trust that these derived SST values are accurately reflecting real ocean surface temperatures? Validation allows us to quantitfy the uncertainty of our product."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90135ee1-bcab-4806-a475-eaa10e4cf2e8",
   "metadata": {},
   "source": [
    "## Finding data to validate with\n",
    "When validating satellite products, we typically want to use in situ measurements: i.e., direct measurements taken in the field. For the ocean, these measurements come from buoys, ship-based thermometers and autonomous floats. An important thing to keep in mind is that while satellites measure the skin temperature (top ~10‚Äì20 microns), in situ platforms measure bulk SST (a few cm to 1 m depth). \n",
    "\n",
    "Should be high quality!\n",
    "\n",
    "Finding a good validation dataset can be tricky. Here's are some lists of commonly-used validation datasets to help get you started: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb65da60-7ccd-45a1-b46e-84caa36a5d29",
   "metadata": {},
   "source": [
    "TO DO: These links need to be checked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a455fd1c-c2cd-4146-88c6-955f470bb9aa",
   "metadata": {},
   "source": [
    "```{admonition} üåä Ocean \n",
    ":class: toggle\n",
    "    \n",
    "| **Dataset**        | **Access**                                                                                                                                                                                                                                                                                                   | **Variables**                     |\n",
    "| ------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | --------------------------------- |\n",
    "| **Argo**           | [argo.ucsd.edu](https://argo.ucsd.edu/data/acknowledging-argo/)  ‚Äî DOI: [10.17882/42182](https://doi.org/10.17882/42182) <br> [NOAA archive](https://www.ncei.noaa.gov/access/metadata/landing-page/bin/iso?id=gov.noaa.nodc%3AArgo-Monthly) ‚Äî DOI: [10.25921/q97e‚Äëd719](https://doi.org/10.25921/q97e-d719) | Sea surface temperature, salinity |\n",
    "| **GHRSST iQuam**   | [GHRSST iQuam website](https://www.star.nesdis.noaa.gov/socd/sst/iquam/) <br> [GHRSST documentation (Zenodo)](https://doi.org/10.5281/zenodo.7589540)                                                                                                                                                        | Sea surface temperature           |\n",
    "| **GTSPP**          | [NCEI GTSPP access](https://www.ncei.noaa.gov/products/global-temperature-salinity-profile-program)                                                                                                                                                                                                          | Temperature, salinity profiles    |\n",
    "| **SPURS**          | [SPURS homepage (NASA)](https://spurs.jpl.nasa.gov/)                                                                                                                                                                                                                                                         | Sea surface salinity              |\n",
    "| **SeaBASS**        | [seabass.gsfc.nasa.gov](https://seabass.gsfc.nasa.gov/)                                                                                                                                                                                                                                                      | Ocean color, chlorophyll-a        |\n",
    "| **NOMAD**          | [NOMAD at NOAA STAR](https://www.star.nesdis.noaa.gov/socd/ocean/color/NOMAD/NOMAD.shtml)                                                                                                                                                                                                                    | Ocean color, chlorophyll-a        |\n",
    "| **BOUSSOLE**       | [BOUSSOLE Project](https://www.obs-vlfr.fr/Boussole/html/home/home.php)                                                                                                                                                                                                                                      | Ocean color, optics, chlorophyll  |\n",
    "| **GDP (Drifters)** | [Global Drifter Program](https://www.aoml.noaa.gov/phod/gdp/)                                                                                                                                                                                                                                                | Ocean surface currents            |\n",
    "| **HF Radar**       | [HFRNet Portal](https://hfrnet.ucsd.edu/)                                                                                                                                                                                                                                                                    | Ocean surface currents            |\n",
    "| **ADCP**           | [NOAA ADCP Program](https://www.aoml.noaa.gov/phod/goos/adcp/index.php)                                                                                                                                                                                                                                      | Water column currents             |\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f40760-4d49-47f4-a6cc-f8d801be0b09",
   "metadata": {},
   "source": [
    "```{admonition} ‚õÖÔ∏è Atmosphere \n",
    ":class: toggle\n",
    "    \n",
    "| **Dataset**    | **Access**                                                                                                                    | **Variables**                             |\n",
    "| -------------- | ----------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------- |\n",
    "| **AERONET**    | [aeronet.gsfc.nasa.gov](https://aeronet.gsfc.nasa.gov/) <br> [re3data entry](https://www.re3data.org/repository/r3d100011742) | Aerosol optical depth (AOD), aerosol type |\n",
    "| **IGRA**       | [IGRA at NOAA](https://www.ncei.noaa.gov/products/integrated-global-radiosonde-archive)                                       | Atmospheric temperature, pressure         |\n",
    "| **GRUAN**      | [GRUAN](https://www.gruan.org/)                                                                                               | Temperature, humidity, pressure           |\n",
    "| **METAR/ASOS** | [Iowa State Mesonet](https://mesonet.agron.iastate.edu/request/download.phtml)                                                | Surface temperature, pressure             |\n",
    "| **GPM-GV**     | [NASA GPM Ground Validation](https://gpm.nasa.gov/resources/ground-validation)                                                | Precipitation                             |\n",
    "| **GPCC**       | [GPCC at DWD](https://opendata.dwd.de/climate_environment/GPCC/)                                                              | Precipitation                             |\n",
    "| **SKYNET**     | [Chiba University SKYNET](http://atmos.cr.chiba-u.ac.jp/skynet/)                                                              | Aerosols, AOD                             |\n",
    "| **MPLNET**     | [MPLNET at NASA](https://mplnet.gsfc.nasa.gov/)                                                                               | Aerosols, clouds                          |\n",
    "| **TCCON**      | [TCCON Data Portal](https://tccondata.org/)                                                                                   | CO‚ÇÇ, CH‚ÇÑ, other trace gases               |\n",
    "| **Pandora**    | [Pandora Project](https://pandora.gsfc.nasa.gov/)                                                                             | NO‚ÇÇ, O‚ÇÉ, trace gases                      |\n",
    "| **MAX-DOAS**   | [MAX-DOAS Network](https://uv-vis.aeronomie.be/maxdoas/)                                                                      | NO‚ÇÇ, SO‚ÇÇ, HCHO                            |\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0744b29-1497-4384-8a53-80428b21268e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "```{admonition} üå≥ Land\n",
    ":class: toggle\n",
    "\n",
    "| **Dataset**  | **Access**                                                           | **Variables**                          |\n",
    "| ------------ | -------------------------------------------------------------------- | -------------------------------------- |\n",
    "| **SURFRAD**  | [NOAA SURFRAD](https://www.esrl.noaa.gov/gmd/grad/surfrad/)          | Land surface temperature, radiation    |\n",
    "| **BSRN**     | [BSRN](https://bsrn.awi.de/)                                         | Solar radiation, surface energy fluxes |\n",
    "| **FLUXNET**  | [fluxnet.org](https://fluxnet.org/)                                  | LST, vegetation indices, fluxes        |\n",
    "| **SCAN**     | [USDA SCAN](https://www.wcc.nrcs.usda.gov/scan/)                     | Soil moisture                          |\n",
    "| **ISMN**     | [International Soil Moisture Network](https://ismn.earth/)           | Soil moisture                          |\n",
    "| **Phenocam** | [Phenocam Network](https://phenocam.sr.unh.edu/)                     | Vegetation phenology (NDVI proxy)      |\n",
    "| **NEON**     | [NEON Data Portal](https://www.neonscience.org/)                     | Vegetation indices, climate variables  |\n",
    "| **BELMANIP** | [Copernicus Land Service](https://land.copernicus.eu/global/sites)   | Leaf area index (LAI)                  |\n",
    "| **VALERI**   | [VALERI Project](https://w3.avignon.inrae.fr/valeri/)                | LAI, fAPAR                             |\n",
    "| **DIRECT**   | [VALERI/DIRECT Info](https://www.avignon.inrae.fr/valeri/?q=node/13) | LAI                                    |\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f9b733-3643-4599-9934-b7c9c3f15261",
   "metadata": {},
   "source": [
    "```{admonition} ‚ùÑÔ∏è Cryosphere \n",
    ":class: toggle\n",
    " \n",
    "| **Dataset**          | **Access**                                                                                                       | **Variables**                        |\n",
    "| -------------------- | ---------------------------------------------------------------------------------------------------------------- | ------------------------------------ |\n",
    "| **SnowEx**           | [NASA SnowEx](https://snow.nasa.gov/campaigns/snowex)                                                            | Snow depth, snow cover, SWE          |\n",
    "| **CALVAL (NSIDC)**   | [NSIDC Cal/Val](https://nsidc.org/data/calval)                                                                   | Snow, cryosphere                     |\n",
    "| **GSNOW**            | [NOAA GSNOW](https://www.ncei.noaa.gov/products/snow-cover)                                                      | Snow cover                           |\n",
    "| **NSIDC**            | [NSIDC](https://nsidc.org/)                                                                                      | Sea ice, snow cover                  |\n",
    "| **IABP (Ice Buoys)** | [International Arctic Buoy Program](https://iabp.apl.uw.edu/)                                                    | Sea ice concentration, drift         |\n",
    "| **IceBridge**        | [NASA IceBridge](https://nsidc.org/data/icebridge)                                                               | Ice sheet elevation, thickness       |\n",
    "| **ATM**              | [Airborne Topographic Mapper (ATM)](https://nsidc.org/data/ILATM2)                                               | Ice elevation profiles               |\n",
    "| **NOHRSC**           | [NOAA NOHRSC](https://www.nohrsc.noaa.gov/)                                                                      | Snow depth and snow water equivalent |\n",
    "| **CCSN (Canada)**    | [Canadian Cryospheric Snow Network](https://open.canada.ca/data/en/dataset/5ff9d50e-591f-4e06-9b8f-0a707e8a1a74) | Snow depth, SWE                      |\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8e68c2-c011-4263-aa53-dc2317ebe57f",
   "metadata": {},
   "source": [
    "### Example: GHRSST iQUAM data\n",
    "For our new SST product, we'll use the GHRSST iQUAM dataset for validation. GHRSST is the Group for High Resolution Sea Surface Temperature ‚Äì an international collaboration supporting high-quality SST products for research and operational use ‚Äì and iQuam is a system developed by NOAA to collect, quality-control (QC), and distribute in situ SST observations in near real-time and delayed mode. This dataset aggregates SST measurements from a variety of in situ platforms, such as drifting buoys, moored buoys, shipboard sensors, and Argo floats. \n",
    "\n",
    "https://www.star.nesdis.noaa.gov/socd/sst/iquam/?tab=0&dateinput_year=2023&dateinput_month=02&dayofmoninput_day=26&dateinput_hour=00&dayofmon=monthly&qcrefsst=_qcrey&qcrefsst=_qccmc&outlier=qced#qmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5bf864-229d-4cbf-bf06-796673d747dd",
   "metadata": {},
   "source": [
    "## Finding data matchups"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1a183ab8-b51c-46dc-993b-6d7feaed159d",
   "metadata": {},
   "source": [
    "Here are some important considerations we need to make when finding matchups between the satellite product and in situ datasets:\n",
    "\n",
    "üîπ **1. Spatial Collocation**\n",
    "- Footprint Differences: Satellite pixels often represent an area (e.g., 1 km¬≤ or more), while in situ data may be point measurements.\n",
    "    - Solution: Use in situ data averaged over the satellite footprint or compare satellite data averaged over multiple pixels surrounding the in situ point.\n",
    "\n",
    "- Geolocation Accuracy: Both satellite and in situ positions should be accurately known, especially in dynamic environments (e.g., drifting buoys).\n",
    "\n",
    "- Environmental Variability: Spatial gradients (e.g., near coastlines or fronts) can lead to mismatches even with close locations.\n",
    "\n",
    "üîπ **2. Temporal Matching**\n",
    "- Temporal Resolution: Satellites provide snapshots (sometimes daily, sometimes instantaneous), while in situ data may be continuous or periodic.\n",
    "    - Solution: Match satellite observation time as closely as possible to in situ sampling time (e.g., within ¬±1 hour).\n",
    "\n",
    "- Diurnal Effects: Some variables (e.g., SST, radiative fluxes) vary significantly throughout the day.\n",
    "    - Solution: Use diurnal correction or only match at known overpass times (e.g., MODIS ~1:30 pm local time).\n",
    "\n",
    "üîπ **3. Variable Definitions and Depths**\n",
    "- Depth Differences: For example, satellite SST represents skin temperature (top ~10‚Äì20 Œºm), whereas in situ sensors often measure at ~1 m.\n",
    "    - Solution: Apply a skin-to-bulk correction or compare to ‚Äúfoundation temperature‚Äù from models.\n",
    "- Variable Representation: Ensure the in situ measurement is of the same quantity the satellite estimates (e.g., top-of-canopy reflectance vs. leaf-level LAI).\n",
    "\n",
    "üîπ **4. Quality Control**\n",
    "- Flagging: Use only high-quality satellite and in situ data (e.g., use quality flags to exclude cloud-contaminated pixels or questionable sensor readings).\n",
    "- Consistency in Units and Calibration: Verify that data are in the same units and reference systems (e.g., radiance vs. reflectance, SI units).\n",
    "- Error Estimates: Consider measurement uncertainty and noise in both datasets.\n",
    "\n",
    "üîπ **5. Statistical Considerations**\n",
    "- Matchup Volume: A large number of matchup pairs increases robustness.\n",
    "- Bias and RMSE Analysis: Use metrics like bias, RMSE, and correlation to assess agreement.\n",
    "- Outlier Handling: Identify and analyze outliers to understand limitations or failure modes.\n",
    "\n",
    "üîπ **6. Sensor Calibration**\n",
    "- Ensure both satellite and in situ instruments are properly calibrated and traceable to standards.\n",
    "\n",
    "üîπ **7. Geophysical Context**\n",
    "- Geographical Diversity: Validation should include diverse regions (e.g., open ocean, coastal, tropical, polar) to capture algorithm performance globally. In our example, we are only validating our product near Antarctica, so we should only use this product in that region. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b458c9d-2f4a-485d-905e-ef2ababfff98",
   "metadata": {},
   "source": [
    "In order to maximize the number of data matchups while still making a fair comparison, we need to determine an appropriate window in space and time. In our example, we set our spatial window to 1 km and our temporal window to half of a day:\n",
    "```\n",
    "dist = 1.0\n",
    "time_add = 0.5\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0c860a-e1cd-4baa-9b90-e5ba6b6e1a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Landsat STAC catalog location\n",
    "url = 'https://landsatlook.usgs.gov/stac-server'\n",
    "\n",
    "# Add Jetstream2 access\n",
    "jetstream_url = 'https://js2.jetstream-cloud.org:8001/'\n",
    "s3 = s3fs.S3FileSystem(anon=True, client_kwargs=dict(endpoint_url=jetstream_url))\n",
    "iQfiles=s3.ls('pythia/landsat8/iQuam') \n",
    "#print(iQfiles)\n",
    "\n",
    "# Paths\n",
    "# note, only the iQuam path will run correctly\n",
    "basepath = Path('/home/jovyan/landsatproduct-cookbook')\n",
    "lsatpath = basepath / 'Data'\n",
    "atmpath = lsatpath / 'AtmCorrection'\n",
    "modout_path = lsatpath / 'MOD07_L2'\n",
    "SSTpath = lsatpath / 'SST/Validation/iQuamIntercomp/'\n",
    "#iQpath = lsatpath / 'iQuam'\n",
    "\n",
    "# Set up the directory structure to hold ERA5 atmospheric profiles and SST data\n",
    "!mkdir Data\n",
    "%cd Data\n",
    "!mkdir iQuam\n",
    "%cd iQuam\n",
    "\n",
    "WV = 'Water_Vapor'\n",
    "\n",
    "# For geopandas and tile plots\n",
    "satellite = 'Landsat8'\n",
    "collection = 'landsat-c2l1' # Landsat Collection 2, Level 1\n",
    "colnm = ['landsat:wrs_path','landsat:wrs_row']\n",
    "gjson_outfile = lsatpath / f'{satellite}_iQuam.geojson'\n",
    "\n",
    "# Buffer around iquam point used to create a bounding box for Landsat sample\n",
    "dist = 1.0 # km\n",
    "\n",
    "# Temporal search range (days) before/after iquam measurement for finding Landsat image\n",
    "time_add = 0.5\n",
    "\n",
    "lthresh = -1.9\n",
    "\n",
    "interp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2527ab54-4b5f-4b74-821e-31a788e0d873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year and months desired (multiple years)\n",
    "start_yr = 2013\n",
    "end_yr = 2023\n",
    "\n",
    "# Note these will get months from the later part of the year to early next\n",
    "start_mo = '09'\n",
    "end_mo = '03'\n",
    "\n",
    "# Headers for the saved outputs\n",
    "headers = ['DateTime','L8_filename','L8_SST','L8_std','center','N','S','E','W','NE','SE','NW','SW','L8_SST_max','L8_SST_min','Argo_id','Argo_SST','A_lat','A_lon']\n",
    "\n",
    "# Desired projection transformation\n",
    "source_crs = 'epsg:4326' \n",
    "target_crs = 'epsg:3031' # Coordinate system of the Landsat file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6a876c-6ad8-4358-b8c5-7059aaf5cfba",
   "metadata": {},
   "source": [
    "Now we're ready to find our matchups!\n",
    "\n",
    "We're repeating the retrieval step for the locations where we have data matchups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb9aca9-57cd-40fa-86c1-2d894d2bbc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Multiple years\n",
    "\n",
    "# Set up projections\n",
    "transformer = Transformer.from_crs(source_crs, target_crs, always_xy=True)\n",
    "\n",
    "# Get iQuam file paths in directory between desired dates and find and produce matching Landsat SSTs \n",
    "for year in range(start_yr, end_yr):  \n",
    "    yrmo = []\n",
    "    start_yrmo = f\"{year}{start_mo}\"  # Start from September of the current year\n",
    "    end_yrmo = f\"{year+1}{end_mo}\"  # End in March of the next year\n",
    "\n",
    "    m0 = start_yrmo\n",
    "    \n",
    "    # Make a list of months between start and end\n",
    "    while int(m0) <= int(end_yrmo):\n",
    "        calc_dt = datetime.strptime(f'{m0[:4]}-{m0[4:]}', '%Y-%m')\n",
    "        yrmo.append(calc_dt.strftime(\"%Y%m\"))\n",
    "        m0 = (calc_dt + relativedelta(months=1)).strftime(\"%Y%m\")\n",
    "    \n",
    "    # Get file names and select only those matching dates from yrmo  \n",
    "    #iQfiles = os.listdir(iQpath)\n",
    "    s3path = 's3://pythia/landsat8/iQuam/*.nc'\n",
    "    iQfiles = s3.glob(s3path)\n",
    "    #s3.invalidate_cache()\n",
    "\n",
    "    #iQfiles = [x for x in iQfiles if x[:6] in yrmo]\n",
    "    iQfiles = [x for x in iQfiles if x[22:22+6] in yrmo]\n",
    "    iQfiles.sort(reverse=True)\n",
    "    print (f'{year}: {len(iQfiles)}')\n",
    "    \n",
    "    #os.chdir(iQpath)\n",
    "\n",
    "    # For each iquam file, pair West Antarctic Argo buoy data with Landsat data and create calibrated SSTs\n",
    "    valid = []\n",
    "    \n",
    "    for iquam_file in iQfiles:\n",
    "        print(iquam_file)\n",
    "        print('s3://'+iquam_file)\n",
    "        \n",
    "        # Open Argos data from iQuam file\n",
    "        s3file = s3.open('s3://'+iquam_file)\n",
    "        \n",
    "        df = xr.open_dataset(s3file)\n",
    "        iquam = df.to_dataframe()\n",
    "        \n",
    "        # Subset to Antarctica\n",
    "        ant = iquam[(iquam.lat<-65)&(iquam.lon>-142)&(iquam.lon<-72)&(iquam.platform_type==5.0)&(iquam.quality_level==5.0)]  # Entire West Antarctica (later)\n",
    "        # ant = iquam[(iquam.lat<-69)&(iquam.lon>-125)&(iquam.lon<-98)&(iquam.platform_type==5.0)&(iquam.quality_level==5.0)] # Amundsen Sea\n",
    "        \n",
    "        # To remove a landsat day that is coming up with a 403 error\n",
    "        if ant['year'].iloc[0] == 2020 and ant['month'].iloc[0] == 12:\n",
    "            ant = ant[ant.day != 9.0] # for 202012 because otherwise will fail\n",
    "        \n",
    "        print('')\n",
    "        print(f'{iquam_file[:6]}: {ant.shape[0]} measurements')\n",
    "    \n",
    "        for idx in tqdm(range(ant.shape[0]), desc=\"Processing\"):\n",
    "    \n",
    "            # Create search area\n",
    "            ilat = ant['lat'].iloc[idx]\n",
    "            ilon = ant['lon'].iloc[idx]\n",
    "    \n",
    "            lat_add = sut.km_to_decimal_degrees(dist, ilat, direction='latitude')\n",
    "            lon_add = sut.km_to_decimal_degrees(dist, ilat, direction='longitude')\n",
    "            bboxV = (ilon-lon_add,ilat-lat_add,ilon+lon_add,ilat+lat_add)\n",
    "    \n",
    "            # Create Landsat temporal search range in correct format\n",
    "            ihr = int(ant.hour.iloc[idx])\n",
    "            iyr = int(ant.year.iloc[idx])\n",
    "            imo = int(ant.month.iloc[idx])\n",
    "    \n",
    "            calc_dt = datetime.strptime(f'{iyr}-{imo}-{int(ant.day.iloc[idx])} {ihr}', '%Y-%m-%d %H')\n",
    "            start_dt = (calc_dt + timedelta(days=-time_add)).strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "            end_dt = (calc_dt + timedelta(days=time_add)).strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "    \n",
    "            timeRangeV = f'{start_dt}/{end_dt}'\n",
    "    \n",
    "            # Search for desired Landsat scenes\n",
    "            items = sut.search_stac(url,collection,gjson_outfile=gjson_outfile,bbox=bboxV,timeRange=timeRangeV)\n",
    "    \n",
    "            # Load the geojson file and open stac catalog\n",
    "            catalog = intake.open_stac_item_collection(items)\n",
    "            gf = gpd.read_file(gjson_outfile)\n",
    "    \n",
    "            # Exclude Landsat 9\n",
    "            catalog_list = [x for x in items if x.id[3]=='8']\n",
    "            num_scene = len(catalog_list)\n",
    "            # print(f'{num_scene} Landsat 8 items')\n",
    "    \n",
    "            # If any matching landsat scenes are found create calibrated SSTs for them\n",
    "            if num_scene>0:\n",
    "    \n",
    "                # Reproject to determine bounding box in espg 3031\n",
    "                sbox,checkbox = sut.lsat_reproj(source_crs,target_crs,(bboxV[0],bboxV[1],bboxV[2],bboxV[3]))\n",
    "    \n",
    "                # Create polygon for later cropping\n",
    "                polygon = Polygon([(sbox[0][0],sbox[0][1]),(sbox[3][0],sbox[3][1]),(sbox[2][0],sbox[2][1]),(sbox[1][0],sbox[1][1])])\n",
    "                \n",
    "                # Create min/max boundaries for trimming image before crop_xarray to cut down on processing times\n",
    "                minx, miny, maxx, maxy = polygon.bounds\n",
    "                polarx = [minx, maxx]\n",
    "                polary = [miny, maxy]\n",
    "    \n",
    "                # Create calibrated SSTs for each matching landsat scene\n",
    "                for sceneid in catalog_list:\n",
    "                    print(sceneid.id)\n",
    "    \n",
    "                    scene = catalog[sceneid.id]\n",
    "                    timestr = scene.metadata['datetime'].strftime('%H%M%S')\n",
    "    \n",
    "                    outFile = f'{SSTpath}/{sceneid.id}_{timestr}_Cel.tif'\n",
    "    \n",
    "                    if os.path.isfile(outFile):\n",
    "                        print (f'{sceneid.id} - atm corr exists')\n",
    "                        ls_scene = xr.open_dataset(outFile,chunks=dict(x=512, y=512),engine='rasterio')\n",
    "                        ls_scene = ls_scene.rio.write_crs(\"epsg:3031\", inplace=True)\n",
    "    \n",
    "                        # Subset all scenes and check for right dimensions because y order changes sometimes\n",
    "                        ls_sub = sut.subset_img(ls_scene['band_data'].sel(band=1),polarx,polary) # subset so easier to work with\n",
    "    \n",
    "                    else:\n",
    "                        # try:\n",
    "                        # Open all desired bands for one scene\n",
    "                        ls_scene = sut.landsat_to_xarray(sceneid,catalog)\n",
    "                        ls_scene = ls_scene.rio.write_crs(\"epsg:3031\", inplace=True)\n",
    "        \n",
    "                        # Create classification mask\n",
    "                        ls_scene = sut.create_masks(ls_scene, cloud_mask=True, ice_mask=True, ocean_mask=True)\n",
    "\n",
    "                        # Check for any open ocean pixels - go to next image if none - ??? s\n",
    "                        mask = np.ones(ls_scene.shape[1:])\n",
    "                        mask[ls_scene.mask!=3] = np.nan\n",
    "                        ls_thermal = ls_scene.sel(band='lwir11').compute()\n",
    "\n",
    "                        # Check in bounding box or for entire Landsat image depending on whether doing calibration runs or not\n",
    "                        ls_box = sut.subset_img(ls_thermal*mask,polarx,polary)\n",
    "\n",
    "                        if ((ls_box).notnull()).sum().values==0:\n",
    "                            print (f'{sceneid.id} has no SSTs')\n",
    "                            try:\n",
    "                                del ls_scene, scene, mask, ls_thermal, ls_box\n",
    "                            except:\n",
    "                                pass\n",
    "                            gc.collect()\n",
    "                            continue\n",
    "\n",
    "                        # Use band ratios for RF cloud pixel classification\n",
    "                        ####\n",
    "\n",
    "                        # Atmospheric correction using MODIS\n",
    "                        # Acquire and align MODIS water vapor (MOD/MYD07) to Landsat\n",
    "                        mod07,modfilenm = sut.open_MODIS(ls_scene,scene,modout_path)\n",
    "                        print(\"1\")\n",
    "\n",
    "                        # Create water vapor files aligned and subsampled to Landsat\n",
    "                        spacing = [300,-300] # 300m sampling of MODIS data so that upsampling is easy and because 30m takes far too long\n",
    "                        WV_xr = sut.get_wv(ls_scene,mod07,spacing,WV,scene,interp=interp)\n",
    "                        print(\"2\")\n",
    "                        # Create SST by masking and using water vapor to apply atmospheric correction\n",
    "                        SST = sut.apply_retrieval(ls_thermal,scene,mask,WV_xr,atmcor,simT_transformer,simTOA_transformer,simWV_transformer)\n",
    "\n",
    "                        # Record MODIS water vapor image used in atmospheric correction, will find this info save under band_data in\n",
    "                        # data variables in COG (click on white paper info button in xarray readout)\n",
    "                        SST.attrs['MODIS_WV'] = modfilenm\n",
    "\n",
    "                        # Save to a cloud-optimized Geotiff\n",
    "                        SST.rio.to_raster(raster_path=outFile, driver=\"COG\")\n",
    "                        print (f'Mean SST: {np.nanmean(SST)}')\n",
    "\n",
    "                        # Subset all scenes and check for right dimensions because y order changes sometimes\n",
    "                        ls_sub = sut.subset_img(SST,polarx,polary) # subset so easier to work with \n",
    "\n",
    "                        try:\n",
    "                            del mask, ls_thermal, mod07, WV_xr, SST\n",
    "                        except:\n",
    "                            pass\n",
    "    \n",
    "                        # except Exception as e:\n",
    "                        #     print (sceneid.id, e)\n",
    "                        #     lsat = np.nan\n",
    "                        #     lstd = np.nan\n",
    "    \n",
    "                    # Crop data to exact bounding box\n",
    "                    ls_sub = sut.crop_xarray_dataarray_with_polygon(ls_sub, polygon) \n",
    "    \n",
    "                    # Calibrate using MODIS\n",
    "                    ls_sub = ls_sub * calib_m + calib_b\n",
    "    \n",
    "                    # Remove all pixels that are too cold\n",
    "                    ls_sub = ls_sub.where(ls_sub>=lthresh,np.nan)\n",
    "    \n",
    "                    lsat = np.around(np.nanmean(ls_sub),2)\n",
    "                    lstd = np.around(np.nanstd(ls_sub),2)\n",
    "    \n",
    "                    # Convert Argo lat/lon to Landsat's EPSG:3031\n",
    "                    argo_px, argo_py = transformer.transform(ilon, ilat)\n",
    "    \n",
    "                    # 1) Find the nearest center pixel\n",
    "                    center_val = ls_sub.sel(x=argo_px, y=argo_py, method=\"nearest\")\n",
    "                    \n",
    "                    # Extract the x/y coordinate values as plain floats\n",
    "                    center_x = center_val.x.item()\n",
    "                    center_y = center_val.y.item()\n",
    "                    \n",
    "                    # 2) Get integer indices from the coordinate indexes\n",
    "                    #    This uses ls_sub.get_index('dim_name') -> pandas.Index -> get_indexer(...)\n",
    "                    center_x_idx = ls_sub.get_index(\"x\").get_indexer([center_x])[0]\n",
    "                    center_y_idx = ls_sub.get_index(\"y\").get_indexer([center_y])[0]\n",
    "                    \n",
    "                    # 3) Gather offsets for the 3x3 neighborhood\n",
    "                    offsets = [\n",
    "                        ( 0,  0, \"center\"),\n",
    "                        ( 0,  1, \"N\"),\n",
    "                        ( 0, -1, \"S\"),\n",
    "                        ( 1,  0, \"E\"),\n",
    "                        (-1,  0, \"W\"),\n",
    "                        ( 1,  1, \"NE\"),\n",
    "                        ( 1, -1, \"SE\"),\n",
    "                        (-1,  1, \"NW\"),\n",
    "                        (-1, -1, \"SW\"),\n",
    "                    ]\n",
    "                    \n",
    "                    neighbors = {}\n",
    "                    for dx, dy, name in offsets:\n",
    "                        nx = center_x_idx + dx\n",
    "                        ny = center_y_idx + dy\n",
    "                        # Ensure we're within the array bounds\n",
    "                        if (0 <= nx < ls_sub.sizes['x']) and (0 <= ny < ls_sub.sizes['y']):\n",
    "                            # xarray dimension order is typically (y, x), so use isel(y=ny, x=nx):\n",
    "                            neighbors[name] = ls_sub.isel(y=ny, x=nx).values.item()\n",
    "                        else:\n",
    "                            neighbors[name] = np.nan\n",
    "                    \n",
    "                    # 4) Record coincident data from Landsat and Argo float\n",
    "                    argo_temp = np.around((ant.sst.iloc[idx] - 273.15),2)  # convert to Celsius\n",
    "                    times = pd.to_datetime(calc_dt, format='%Y%m%d%H')  # standardize time\n",
    "                    \n",
    "                    valid.append([\n",
    "                        times,\n",
    "                        sceneid.id,\n",
    "                        lsat,\n",
    "                        lstd,\n",
    "                        neighbors['center'],\n",
    "                        neighbors['N'],\n",
    "                        neighbors['S'],\n",
    "                        neighbors['E'],\n",
    "                        neighbors['W'],\n",
    "                        neighbors['NE'],\n",
    "                        neighbors['SE'],\n",
    "                        neighbors['NW'],\n",
    "                        neighbors['SW'],\n",
    "                        ls_sub.max().values.item(),\n",
    "                        ls_sub.min().values.item(),\n",
    "                        ant.iloc[idx].name,  # Argo ID or whichever label you prefer\n",
    "                        argo_temp,\n",
    "                        ilat,\n",
    "                        ilon\n",
    "                    ])\n",
    "                    print (f'Argo temp: {argo_temp}, Landsat 8 mean: {lsat}+/-{lstd}')\n",
    "    \n",
    "                    try:\n",
    "                        del ls_scene, scene, ls_thermal, ls_box, mod07, WV_xr, SST, ls_sub, neighbors\n",
    "                    except:\n",
    "                        pass\n",
    "    \n",
    "                    gc.collect()\n",
    "\n",
    "    # Put data into DataFrame and save    \n",
    "    lsat_mod_df = pd.DataFrame(valid,columns=headers)\n",
    "    out_df = f'Landsat_validation_{start_yrmo}_{end_yrmo}_{dist}.csv'\n",
    "    lsat_mod_df.to_csv(out_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13eacc2c-ab93-4df3-8afa-418f77041d05",
   "metadata": {},
   "source": [
    "### Landsat-iQUAM validation assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b61cb9-8146-46d6-ace8-112e67fc7eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nothing has been tested below this point after accessing iQuam data from Jetstream\n",
    "# KM 8/8/25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c9d945-424a-4749-98e3-49b3fc216ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in processed matchups\n",
    "\n",
    "# 20250107 is threshold=-1.9, 20250114 is thresh=-1.94, 20250205 is thresh=-1.9\n",
    "out_df = lsatpath / 'Landsat_validation_202009_202103_1.0.csv'\n",
    "valids = pd.read_csv(out_df)\n",
    "\n",
    "valids = valids.set_index('DateTime')\n",
    "valids = valids.sort_index()\n",
    "valids['Argo_id'] = valids['Argo_id'].astype(int)\n",
    "\n",
    "# Remove the non-numeric column for calculating daily means\n",
    "numeric_valids = valids.select_dtypes(include=[np.number])\n",
    "validmn = numeric_valids.groupby(numeric_valids['Argo_id']).mean()\n",
    "valids = valids.reset_index()\n",
    "\n",
    "# Group by the date component of the datetime and calculate the difference\n",
    "valids['temp_dif'] = valids.groupby(valids['Argo_id'])[f'L8_SST'].diff()\n",
    "validmn['temp_dif'] = valids.groupby(valids['Argo_id'])['temp_dif'].first()\n",
    "valids = valids.set_index('DateTime')\n",
    "\n",
    "validmn['std'] = valids.groupby([valids['Argo_id']])[f'L8_SST'].std()\n",
    "validmn['xaxis'] = pd.to_datetime(validmn.index).dayofyear\n",
    "validmn['xaxis'][validmn['xaxis']<(365/2)] = validmn['xaxis'] + 365\n",
    "\n",
    "valids['xaxis'] = pd.to_datetime(valids.index).dayofyear\n",
    "valids['xaxis'][valids['xaxis']<(365/2)] = valids['xaxis'] + 365\n",
    "\n",
    "validmn = validmn.dropna(subset=['L8_SST'])\n",
    "\n",
    "valids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fe69e4-a23e-47d5-9108-e5dbc821ae5b",
   "metadata": {},
   "source": [
    "### Visualize all validation matchups for manual QC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607b42a7-4f5f-422d-9748-69df0ead7766",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setup and authenticate \n",
    "from dask.distributed import Client\n",
    "import logging\n",
    "client = Client(processes=True, n_workers=4, \n",
    "                threads_per_worker=1,\n",
    "                silence_logs=logging.ERROR)\n",
    "client.run(lambda: os.environ[\"AWS_REQUEST_PAYER\"] == \"requester\" )\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2977e506-8e86-4263-925f-c3d4c6fcefeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de6c02a-740c-4025-9119-2e95d36fc29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(lsatpath)\n",
    "lsatfiles = os.listdir(lsatpath)\n",
    "lsatfiles = [i for i in lsatfiles if i[0]=='L']\n",
    "\n",
    "# Remove all files from the list with repeats and Landsat mean = nan\n",
    "remove = [\n",
    "'LC08_L1GT_029109_20160922_20200906_02_T2', \n",
    "'LC08_L1GT_022110_20181029_20201016_02_T2', \n",
    "'LC08_L1GT_005110_20181209_20201016_02_T2',\n",
    "'LC08_L1GT_228108_20200123_20201016_02_T2',\n",
    "'LC08_L1GT_001108_20200929_20201006_02_T2',\n",
    "'LC08_L1GT_002109_20201022_20201105_02_T2',\n",
    "'LC08_L1GT_233108_20201109_20210317_02_T2',\n",
    "'LC08_L1GT_233109_20201109_20210317_02_T2',\n",
    "'LC08_L1GT_001109_20201202_20210312_02_T2',\n",
    "'LC08_L1GT_027112_20220128_20220204_02_T2',\n",
    "'LC08_L1GT_002109_20210331_20210408_02_T2'\n",
    "]\n",
    "\n",
    "lsatfiles = [i for i in lsatfiles if i[:-15] not in remove]\n",
    "if len(lsatfiles)!=12:\n",
    "    print('Wrong number of Landsat scenes!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3725c3-2885-4faa-84f4-1f6768de41d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of columns/rows for subplots\n",
    "n_cols = 3\n",
    "n_rows = 4\n",
    "\n",
    "# Create one figure and a 4x7 grid of subplots\n",
    "# The figsize is 10 across; adjust height as needed for clarity\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=n_rows,\n",
    "    ncols=n_cols,\n",
    "    figsize=(11, 8.5),   \n",
    "    subplot_kw={'projection': ccrs.PlateCarree()}\n",
    ")\n",
    "\n",
    "# Optional: if you want them *really* close, you can fine-tune spacing:\n",
    "plt.subplots_adjust(wspace=0.05, hspace=0.03)\n",
    "# plt.subplots_adjust(bottom=0.15)\n",
    "\n",
    "# Counter for subplot index, and a handle to store the last \"imshow\" (for colorbar)\n",
    "i = 0\n",
    "im_obj = None\n",
    "\n",
    "for lsatfile in lsatfiles:\n",
    "    lsID = lsatfile\n",
    "    print(lsID)\n",
    "    \n",
    "    mrow = valids[valids['L8_filename'].str.contains(lsatfile[:-15])]\n",
    "    \n",
    "    for idx, row in mrow.iterrows():\n",
    "        # Check if L8_SST is NaN\n",
    "        if pd.isnull(row['L8_SST']):\n",
    "            # If it is NaN, skip this iteration and do not plot\n",
    "            continue\n",
    "        \n",
    "        # --- Prepare data and coordinates ---\n",
    "        ilat = row['A_lat']\n",
    "        ilon = row['A_lon']\n",
    "        \n",
    "        lat_add = km_to_decimal_degrees(dist, ilat, direction='latitude')\n",
    "        lon_add = km_to_decimal_degrees(dist, ilat, direction='longitude')\n",
    "        xmin, ymin, xmax, ymax = (ilon - lon_add, ilat - lat_add, \n",
    "                                  ilon + lon_add, ilat + lat_add)\n",
    "        \n",
    "        # Load the Landsat file\n",
    "        ds = xr.open_dataset(lsatfile, chunks=dict(x=512, y=512), engine='rasterio')\n",
    "        ls_scene = ds['band_data'].sel(band=1).rio.write_crs(\"epsg:3031\", inplace=True)\n",
    "        \n",
    "        # Assign time coordinate\n",
    "        times = pd.to_datetime(lsatfile[17:25] + lsatfile[41:47], format='%Y%m%d%H%M%S')\n",
    "        ls_scene = ls_scene.assign_coords(time=times, ID=lsatfile[:-8])\n",
    "        \n",
    "        # Reproject to EPSG:4326\n",
    "        ls_scene = ls_scene.rio.reproject(\"EPSG:4326\")\n",
    "\n",
    "        # Calibrate using MODIS\n",
    "        ls_scene = ls_scene * calib_m + calib_b\n",
    "\n",
    "        # Remove all pixels that are too cold\n",
    "        ls_scene = ls_scene.where(ls_scene>=lthresh,np.nan)\n",
    "        \n",
    "        # Select the subplot axis\n",
    "        ax = axes[i // n_cols, i % n_cols]\n",
    "        \n",
    "        # Plot on that axis, without a colorbar\n",
    "        # Store the \"imshow\" result in im_obj so we can build one colorbar later\n",
    "        im_obj = ls_scene.plot.imshow(\n",
    "            x='x', y='y',\n",
    "            vmin=-2.0, vmax=1.5,\n",
    "            ax=ax,\n",
    "            transform=ccrs.PlateCarree(),\n",
    "            origin='upper',\n",
    "            add_colorbar=False  # <- No individual colorbar\n",
    "        )\n",
    "        \n",
    "        # Remove titles completely (xarray may add one by default)\n",
    "        ax.set_title('')\n",
    "        \n",
    "        # Set extent\n",
    "        ax.set_extent([ilon - 0.4, ilon + 0.4, ilat - 0.2, ilat + 0.2], crs=ccrs.PlateCarree())\n",
    "        \n",
    "        # Argo observation\n",
    "        ax.scatter([ilon], [ilat], c='r', s=3, transform=ccrs.PlateCarree(), label='Argo location')\n",
    "        \n",
    "        # Draw bounding box\n",
    "        polygon_show = Pgon([(xmin, ymin), (xmin, ymax), \n",
    "                             (xmax, ymax), (xmax, ymin)],\n",
    "                            closed=True, edgecolor='red', facecolor='none')\n",
    "        ax.add_patch(polygon_show)\n",
    "        \n",
    "        # Text label\n",
    "        text_str = (\n",
    "            f\"Argo ID: {row['Argo_id']}\\n\"\n",
    "            f\"Argo Temp: {np.around(row['Argo_SST'], 2)}\\n\"\n",
    "            f\"Landsat SST: {np.around(row['L8_SST'], 2)}\"\n",
    "        )\n",
    "        ax.text(\n",
    "            0.02, 0.95,\n",
    "            text_str,\n",
    "            transform=ax.transAxes,\n",
    "            fontsize=10,\n",
    "            va='top',\n",
    "            ha='left'\n",
    "        )\n",
    "\n",
    "        gc.collect()\n",
    "        \n",
    "        # Move to next subplot index\n",
    "        i += 1\n",
    "        \n",
    "\n",
    "# --- Add a single colorbar for the entire figure ---\n",
    "# We use the last \"imshow\" (im_obj) and attach to all subplot axes\n",
    "cbar_ax = fig.add_axes([0.25, 0.05, 0.5, 0.03])\n",
    "\n",
    "cbar = fig.colorbar(\n",
    "    im_obj, \n",
    "    ax=axes.ravel().tolist(),  # or just ax=axes if axes is 2D\n",
    "    cax=cbar_ax,\n",
    "    orientation='horizontal',  # 'vertical' or 'horizontal'\n",
    "    fraction=0.025,            # how long the colorbar is relative to axes\n",
    "    pad=0.05                   # space between colorbar and subplots\n",
    ")\n",
    "cbar.set_label(\"Temperature [¬∞C]\", fontsize=12)\n",
    "\n",
    "# plt.tight_layout()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6366390a-5c68-4101-b5bb-306a538a048d",
   "metadata": {},
   "source": [
    "# Use matchups to compare modeled and observed data\n",
    "Here, we are going to use a linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52881003-d49a-40f8-bb31-3b1379960a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove low quality validation data from the valids data\n",
    "rm_ids = [1460597, 1614080, 2143782, 2016236, 1790883]\n",
    "rm = validmn[validmn.index.isin(rm_ids)]\n",
    "validmn = validmn.drop(index=rm_ids, errors='ignore')\n",
    "valids = valids[~valids['Argo_id'].isin(rm_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9832c9-be57-4022-90f5-ca22496eca7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Orthoganal Regression \n",
    "data = validmn\n",
    "\n",
    "# Original data\n",
    "x_original = np.array(data['Argo_SST'])\n",
    "y_original = np.array(data['L8_SST'])\n",
    "\n",
    "# Define a linear function for the model\n",
    "def linear_model(p, x):\n",
    "    return p[0] * x + p[1]\n",
    "\n",
    "# Create a Model\n",
    "linear = Model(linear_model)\n",
    "\n",
    "# Create a RealData object using your DataFrame\n",
    "data = RealData(x_original, y_original)\n",
    "\n",
    "# Set up ODR with the model and data\n",
    "odr = ODR(data, linear, beta0=[1., 0.])\n",
    "\n",
    "# Run the regression\n",
    "out = odr.run()\n",
    "\n",
    "# Use the output\n",
    "beta = out.beta\n",
    "beta_err = out.sd_beta\n",
    "\n",
    "# Print the summary\n",
    "out.pprint()\n",
    "\n",
    "# Predicting values using the ODR model\n",
    "y_pred = linear_model(beta, x_original)\n",
    "\n",
    "# Get R2\n",
    "# Calculate Total Sum of Squares (SST)\n",
    "y_mean = np.mean(y_original)\n",
    "SST = np.sum((y_original - y_mean)**2)\n",
    "\n",
    "# Calculate Residual Sum of Squares (SSR)\n",
    "SSR = np.sum((y_original - y_pred)**2)\n",
    "\n",
    "# Compute RMSE\n",
    "rmse = np.sqrt(((y_original - y_pred) ** 2).mean())\n",
    "\n",
    "# Calculate R^2\n",
    "R2 = 1 - (SSR / SST)\n",
    "print(\"R^2:\", np.around(R2,2))\n",
    "print(f\"RMSE: {np.around(rmse,2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe231c0-2207-4c2a-8f9f-c1afa7ecd923",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "beta_mdn = [beta[0]-beta_err[0]*1.96,beta[1]-beta_err[1]*1.96]\n",
    "beta_mup = [beta[0]+beta_err[0]*1.96,beta[1]-beta_err[1]*1.96]\n",
    "beta_bdn = [beta[0]-beta_err[0]*1.96,beta[1]+beta_err[1]*1.96]\n",
    "beta_bup = [beta[0]+beta_err[0]*1.96,beta[1]+beta_err[1]*1.96]\n",
    "print(f'At 95% confidence interval: {np.around(beta[0],2)}+/-{np.around(beta_err[0]*1.96,2)}, {np.around(beta[1],2)}+/-{np.around(beta_err[1]*1.96,2)}, n={y_pred.shape[0]}')\n",
    "xfill = np.array([-4.3,0.9])\n",
    "\n",
    "\n",
    "# Plot data points and 1:1 line\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "ax.tick_params(labelsize=14)\n",
    "\n",
    "xi = np.arange(-7.0,5.0,1.0)\n",
    "\n",
    "# lower_err = abs(data['L8_SST'] - data['L8_SST_min'])  # distance to lower bound\n",
    "# upper_err = abs(data['L8_SST_max'] - data['L8_SST'])  # distance to upper bound\n",
    "\n",
    "ax.scatter(x_original,y_original,color='k',linewidth=0,s=35,label='High quality')\n",
    "ax.errorbar(x_original,y_original,yerr=validmn['std'],color='k',fmt='o',ecolor='k',elinewidth=1,capthick=1,marker='o',ms=3,capsize=5,label='_no legend_')\n",
    "# ax.errorbar(data['Argo_SST'],data['L8_SST'],yerr=[lower_err,upper_err],color='k',fmt='o',ecolor='k',elinewidth=1,capthick=1,marker='o',ms=3,capsize=5,label='_no legend_')\n",
    "# ax.scatter(data['Argo_SST'],data['center'],color='r',linewidth=0,s=25,label='_no label_')\n",
    "ax.scatter(rm['Argo_SST'],rm['L8_SST'],color='0.7', s=35, label='Removed',zorder=2)\n",
    "ax.plot(xi,xi,color='k',linewidth=2, label='1:1')\n",
    "ax.plot(x_original, y_pred, color='k', ls=':', label='Validation ODR')\n",
    "ax.fill_between(xfill, linear_model(beta_bdn, xfill), linear_model(beta_mup, xfill),alpha=0.1, facecolor='0.3')\n",
    "# a1.plot(xi,xi*lreg.coef_[0]+lreg.intercept_[0],color='r',linewidth=2,label='RANSAC regression')\n",
    "# a1.scatter(xC,yLC,color='r',linewidth=0,s=35,label='_no label_')\n",
    "# a1.plot(xi,xi*resultC.params.L8_SST+resultC.params.Intercept,color='k',linewidth=2,label='OLS regression')\n",
    "# a1.text(-2.1,-2.85,f'y={np.around(resultC.params.L8_SST,2)}x+{np.around(resultC.params.Intercept,2)}   p={p_val}',fontsize=12)\n",
    "ax.text(-1.1,-1.5,f'y={np.around(beta[0],2)}x+{np.around(beta[1],2)}   $r^2$={np.around(R2,2)}',fontsize=14)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.set_ylim([-1.95,1.5])\n",
    "ax.set_xlim([-1.95,1.5])\n",
    "ax.set_xlabel('Argo Temperature [¬∞C]',fontsize=16)\n",
    "ax.set_ylabel('Landsat SST [¬∞C]',fontsize=16)\n",
    "ax.legend(markerscale=1,fontsize=12,loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig('/Users/tsnow03/GoogleDrive/User/Docs/PhD_Project/Manuscripts/AmundsenCC_Manuscript/Figures/LMCalibration.jpg', format='jpg', dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8015d2-fd8f-4a40-8b3c-0c06d03e6bb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
