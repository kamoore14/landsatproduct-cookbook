{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be0eb869-631f-4f0a-8ddd-b61b2e9c0331",
   "metadata": {},
   "source": [
    "---\n",
    "title: Validating your data product \n",
    "authors: \n",
    "- name: Genevive Clow\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68739192-2d47-4b83-9d94-2c8c4fa447fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install xarray==2024.05.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98b989a0-346a-4949-83d3-074be8f1ff9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "#%pip install pykrige\n",
    "#%pip install xarray\n",
    "# %pip install xarray==2024.05.0\n",
    "\n",
    "%matplotlib widget\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d719bcd8-23d0-4ef5-bc15-5e56e83e4f9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"esms-options\">{\"shimMode\": true}</script><style>*[data-root-id],\n",
       "*[data-root-id] > * {\n",
       "  box-sizing: border-box;\n",
       "  font-family: var(--jp-ui-font-family);\n",
       "  font-size: var(--jp-ui-font-size1);\n",
       "  color: var(--vscode-editor-foreground, var(--jp-ui-font-color1));\n",
       "}\n",
       "\n",
       "/* Override VSCode background color */\n",
       ".cell-output-ipywidget-background:has(\n",
       "  > .cell-output-ipywidget-background > .lm-Widget > *[data-root-id]\n",
       "),\n",
       ".cell-output-ipywidget-background:has(> .lm-Widget > *[data-root-id]) {\n",
       "  background-color: transparent !important;\n",
       "}\n",
       "</style>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  const force = true;\n",
       "  const py_version = '3.7.3'.replace('rc', '-rc.').replace('.dev', '-dev.');\n",
       "  const reloading = false;\n",
       "  const Bokeh = root.Bokeh;\n",
       "\n",
       "  // Set a timeout for this load but only if we are not already initializing\n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || (force || !root._bokeh_is_initializing)) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks;\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, js_modules, js_exports, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "    if (js_modules == null) js_modules = [];\n",
       "    if (js_exports == null) js_exports = {};\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      // Don't load bokeh if it is still initializing\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    } else if (js_urls.length === 0 && js_modules.length === 0 && Object.keys(js_exports).length === 0) {\n",
       "      // There is nothing to load\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "    window._bokeh_on_load = on_load\n",
       "\n",
       "    function on_error(e) {\n",
       "      const src_el = e.srcElement\n",
       "      console.error(\"failed to load \" + (src_el.href || src_el.src));\n",
       "    }\n",
       "\n",
       "    const skip = [];\n",
       "    if (window.requirejs) {\n",
       "      window.requirejs.config({'packages': {}, 'paths': {}, 'shim': {}});\n",
       "      root._bokeh_is_loading = css_urls.length + 0;\n",
       "    } else {\n",
       "      root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length + Object.keys(js_exports).length;\n",
       "    }\n",
       "\n",
       "    const existing_stylesheets = []\n",
       "    const links = document.getElementsByTagName('link')\n",
       "    for (let i = 0; i < links.length; i++) {\n",
       "      const link = links[i]\n",
       "      if (link.href != null) {\n",
       "        existing_stylesheets.push(link.href)\n",
       "      }\n",
       "    }\n",
       "    for (let i = 0; i < css_urls.length; i++) {\n",
       "      const url = css_urls[i];\n",
       "      const escaped = encodeURI(url)\n",
       "      if (existing_stylesheets.indexOf(escaped) !== -1) {\n",
       "        on_load()\n",
       "        continue;\n",
       "      }\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }    var existing_scripts = []\n",
       "    const scripts = document.getElementsByTagName('script')\n",
       "    for (let i = 0; i < scripts.length; i++) {\n",
       "      var script = scripts[i]\n",
       "      if (script.src != null) {\n",
       "        existing_scripts.push(script.src)\n",
       "      }\n",
       "    }\n",
       "    for (let i = 0; i < js_urls.length; i++) {\n",
       "      const url = js_urls[i];\n",
       "      const escaped = encodeURI(url)\n",
       "      if (skip.indexOf(escaped) !== -1 || existing_scripts.indexOf(escaped) !== -1) {\n",
       "        if (!window.requirejs) {\n",
       "          on_load();\n",
       "        }\n",
       "        continue;\n",
       "      }\n",
       "      const element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "    for (let i = 0; i < js_modules.length; i++) {\n",
       "      const url = js_modules[i];\n",
       "      const escaped = encodeURI(url)\n",
       "      if (skip.indexOf(escaped) !== -1 || existing_scripts.indexOf(escaped) !== -1) {\n",
       "        if (!window.requirejs) {\n",
       "          on_load();\n",
       "        }\n",
       "        continue;\n",
       "      }\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      element.type = \"module\";\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "    for (const name in js_exports) {\n",
       "      const url = js_exports[name];\n",
       "      const escaped = encodeURI(url)\n",
       "      if (skip.indexOf(escaped) >= 0 || root[name] != null) {\n",
       "        if (!window.requirejs) {\n",
       "          on_load();\n",
       "        }\n",
       "        continue;\n",
       "      }\n",
       "      var element = document.createElement('script');\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.type = \"module\";\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      element.textContent = `\n",
       "      import ${name} from \"${url}\"\n",
       "      window.${name} = ${name}\n",
       "      window._bokeh_on_load()\n",
       "      `\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "    if (!js_urls.length && !js_modules.length) {\n",
       "      on_load()\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  const js_urls = [\"https://cdn.holoviz.org/panel/1.7.5/dist/bundled/reactiveesm/es-module-shims@^1.10.0/dist/es-module-shims.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-3.7.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.7.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.7.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.7.3.min.js\", \"https://cdn.holoviz.org/panel/1.7.5/dist/panel.min.js\", \"https://cdn.jsdelivr.net/npm/@holoviz/geoviews@1.14.0/dist/geoviews.min.js\"];\n",
       "  const js_modules = [];\n",
       "  const js_exports = {};\n",
       "  const css_urls = [];\n",
       "  const inline_js = [    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "function(Bokeh) {} // ensure no trailing comma for IE\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (let i = 0; i < inline_js.length; i++) {\n",
       "        try {\n",
       "          inline_js[i].call(root, root.Bokeh);\n",
       "        } catch(e) {\n",
       "          if (!reloading) {\n",
       "            throw e;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      // Cache old bokeh versions\n",
       "      if (Bokeh != undefined && !reloading) {\n",
       "        var NewBokeh = root.Bokeh;\n",
       "        if (Bokeh.versions === undefined) {\n",
       "          Bokeh.versions = new Map();\n",
       "        }\n",
       "        if (NewBokeh.version !== Bokeh.version) {\n",
       "          Bokeh.versions.set(NewBokeh.version, NewBokeh)\n",
       "        }\n",
       "        root.Bokeh = Bokeh;\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    }\n",
       "    root._bokeh_is_initializing = false\n",
       "  }\n",
       "\n",
       "  function load_or_wait() {\n",
       "    // Implement a backoff loop that tries to ensure we do not load multiple\n",
       "    // versions of Bokeh and its dependencies at the same time.\n",
       "    // In recent versions we use the root._bokeh_is_initializing flag\n",
       "    // to determine whether there is an ongoing attempt to initialize\n",
       "    // bokeh, however for backward compatibility we also try to ensure\n",
       "    // that we do not start loading a newer (Panel>=1.0 and Bokeh>3) version\n",
       "    // before older versions are fully initialized.\n",
       "    if (root._bokeh_is_initializing && Date.now() > root._bokeh_timeout) {\n",
       "      // If the timeout and bokeh was not successfully loaded we reset\n",
       "      // everything and try loading again\n",
       "      root._bokeh_timeout = Date.now() + 5000;\n",
       "      root._bokeh_is_initializing = false;\n",
       "      root._bokeh_onload_callbacks = undefined;\n",
       "      root._bokeh_is_loading = 0\n",
       "      console.log(\"Bokeh: BokehJS was loaded multiple times but one version failed to initialize.\");\n",
       "      load_or_wait();\n",
       "    } else if (root._bokeh_is_initializing || (typeof root._bokeh_is_initializing === \"undefined\" && root._bokeh_onload_callbacks !== undefined)) {\n",
       "      setTimeout(load_or_wait, 100);\n",
       "    } else {\n",
       "      root._bokeh_is_initializing = true\n",
       "      root._bokeh_onload_callbacks = []\n",
       "      const bokeh_loaded = root.Bokeh != null && (root.Bokeh.version === py_version || (root.Bokeh.versions !== undefined && root.Bokeh.versions.has(py_version)));\n",
       "      if (!reloading && !bokeh_loaded) {\n",
       "        if (root.Bokeh) {\n",
       "          root.Bokeh = undefined;\n",
       "        }\n",
       "        console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "      }\n",
       "      load_libs(css_urls, js_urls, js_modules, js_exports, function() {\n",
       "        console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "        run_inline_js();\n",
       "      });\n",
       "    }\n",
       "  }\n",
       "  // Give older versions of the autoload script a head-start to ensure\n",
       "  // they initialize before we start loading newer version.\n",
       "  setTimeout(load_or_wait, 100)\n",
       "}(window));"
      ],
      "application/vnd.holoviews_load.v0+json": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n  const py_version = '3.7.3'.replace('rc', '-rc.').replace('.dev', '-dev.');\n  const reloading = false;\n  const Bokeh = root.Bokeh;\n\n  // Set a timeout for this load but only if we are not already initializing\n  if (typeof (root._bokeh_timeout) === \"undefined\" || (force || !root._bokeh_is_initializing)) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks;\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, js_modules, js_exports, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n    if (js_modules == null) js_modules = [];\n    if (js_exports == null) js_exports = {};\n\n    root._bokeh_onload_callbacks.push(callback);\n\n    if (root._bokeh_is_loading > 0) {\n      // Don't load bokeh if it is still initializing\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    } else if (js_urls.length === 0 && js_modules.length === 0 && Object.keys(js_exports).length === 0) {\n      // There is nothing to load\n      run_callbacks();\n      return null;\n    }\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n    window._bokeh_on_load = on_load\n\n    function on_error(e) {\n      const src_el = e.srcElement\n      console.error(\"failed to load \" + (src_el.href || src_el.src));\n    }\n\n    const skip = [];\n    if (window.requirejs) {\n      window.requirejs.config({'packages': {}, 'paths': {}, 'shim': {}});\n      root._bokeh_is_loading = css_urls.length + 0;\n    } else {\n      root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length + Object.keys(js_exports).length;\n    }\n\n    const existing_stylesheets = []\n    const links = document.getElementsByTagName('link')\n    for (let i = 0; i < links.length; i++) {\n      const link = links[i]\n      if (link.href != null) {\n        existing_stylesheets.push(link.href)\n      }\n    }\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const escaped = encodeURI(url)\n      if (existing_stylesheets.indexOf(escaped) !== -1) {\n        on_load()\n        continue;\n      }\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }    var existing_scripts = []\n    const scripts = document.getElementsByTagName('script')\n    for (let i = 0; i < scripts.length; i++) {\n      var script = scripts[i]\n      if (script.src != null) {\n        existing_scripts.push(script.src)\n      }\n    }\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const escaped = encodeURI(url)\n      if (skip.indexOf(escaped) !== -1 || existing_scripts.indexOf(escaped) !== -1) {\n        if (!window.requirejs) {\n          on_load();\n        }\n        continue;\n      }\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (let i = 0; i < js_modules.length; i++) {\n      const url = js_modules[i];\n      const escaped = encodeURI(url)\n      if (skip.indexOf(escaped) !== -1 || existing_scripts.indexOf(escaped) !== -1) {\n        if (!window.requirejs) {\n          on_load();\n        }\n        continue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (const name in js_exports) {\n      const url = js_exports[name];\n      const escaped = encodeURI(url)\n      if (skip.indexOf(escaped) >= 0 || root[name] != null) {\n        if (!window.requirejs) {\n          on_load();\n        }\n        continue;\n      }\n      var element = document.createElement('script');\n      element.onerror = on_error;\n      element.async = false;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      element.textContent = `\n      import ${name} from \"${url}\"\n      window.${name} = ${name}\n      window._bokeh_on_load()\n      `\n      document.head.appendChild(element);\n    }\n    if (!js_urls.length && !js_modules.length) {\n      on_load()\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.holoviz.org/panel/1.7.5/dist/bundled/reactiveesm/es-module-shims@^1.10.0/dist/es-module-shims.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-3.7.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.7.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.7.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.7.3.min.js\", \"https://cdn.holoviz.org/panel/1.7.5/dist/panel.min.js\", \"https://cdn.jsdelivr.net/npm/@holoviz/geoviews@1.14.0/dist/geoviews.min.js\"];\n  const js_modules = [];\n  const js_exports = {};\n  const css_urls = [];\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (let i = 0; i < inline_js.length; i++) {\n        try {\n          inline_js[i].call(root, root.Bokeh);\n        } catch(e) {\n          if (!reloading) {\n            throw e;\n          }\n        }\n      }\n      // Cache old bokeh versions\n      if (Bokeh != undefined && !reloading) {\n        var NewBokeh = root.Bokeh;\n        if (Bokeh.versions === undefined) {\n          Bokeh.versions = new Map();\n        }\n        if (NewBokeh.version !== Bokeh.version) {\n          Bokeh.versions.set(NewBokeh.version, NewBokeh)\n        }\n        root.Bokeh = Bokeh;\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    }\n    root._bokeh_is_initializing = false\n  }\n\n  function load_or_wait() {\n    // Implement a backoff loop that tries to ensure we do not load multiple\n    // versions of Bokeh and its dependencies at the same time.\n    // In recent versions we use the root._bokeh_is_initializing flag\n    // to determine whether there is an ongoing attempt to initialize\n    // bokeh, however for backward compatibility we also try to ensure\n    // that we do not start loading a newer (Panel>=1.0 and Bokeh>3) version\n    // before older versions are fully initialized.\n    if (root._bokeh_is_initializing && Date.now() > root._bokeh_timeout) {\n      // If the timeout and bokeh was not successfully loaded we reset\n      // everything and try loading again\n      root._bokeh_timeout = Date.now() + 5000;\n      root._bokeh_is_initializing = false;\n      root._bokeh_onload_callbacks = undefined;\n      root._bokeh_is_loading = 0\n      console.log(\"Bokeh: BokehJS was loaded multiple times but one version failed to initialize.\");\n      load_or_wait();\n    } else if (root._bokeh_is_initializing || (typeof root._bokeh_is_initializing === \"undefined\" && root._bokeh_onload_callbacks !== undefined)) {\n      setTimeout(load_or_wait, 100);\n    } else {\n      root._bokeh_is_initializing = true\n      root._bokeh_onload_callbacks = []\n      const bokeh_loaded = root.Bokeh != null && (root.Bokeh.version === py_version || (root.Bokeh.versions !== undefined && root.Bokeh.versions.has(py_version)));\n      if (!reloading && !bokeh_loaded) {\n        if (root.Bokeh) {\n          root.Bokeh = undefined;\n        }\n        console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n      }\n      load_libs(css_urls, js_urls, js_modules, js_exports, function() {\n        console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n        run_inline_js();\n      });\n    }\n  }\n  // Give older versions of the autoload script a head-start to ensure\n  // they initialize before we start loading newer version.\n  setTimeout(load_or_wait, 100)\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "if ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n",
       "  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n",
       "}\n",
       "\n",
       "\n",
       "    function JupyterCommManager() {\n",
       "    }\n",
       "\n",
       "    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n",
       "      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n",
       "        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n",
       "        comm_manager.register_target(comm_id, function(comm) {\n",
       "          comm.on_msg(msg_handler);\n",
       "        });\n",
       "      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n",
       "        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n",
       "          comm.onMsg = msg_handler;\n",
       "        });\n",
       "      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n",
       "        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n",
       "          var messages = comm.messages[Symbol.asyncIterator]();\n",
       "          function processIteratorResult(result) {\n",
       "            var message = result.value;\n",
       "            var content = {data: message.data, comm_id};\n",
       "            var buffers = []\n",
       "            for (var buffer of message.buffers || []) {\n",
       "              buffers.push(new DataView(buffer))\n",
       "            }\n",
       "            var metadata = message.metadata || {};\n",
       "            var msg = {content, buffers, metadata}\n",
       "            msg_handler(msg);\n",
       "            return messages.next().then(processIteratorResult);\n",
       "          }\n",
       "          return messages.next().then(processIteratorResult);\n",
       "        })\n",
       "      }\n",
       "    }\n",
       "\n",
       "    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n",
       "      if (comm_id in window.PyViz.comms) {\n",
       "        return window.PyViz.comms[comm_id];\n",
       "      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n",
       "        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n",
       "        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n",
       "        if (msg_handler) {\n",
       "          comm.on_msg(msg_handler);\n",
       "        }\n",
       "      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n",
       "        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n",
       "        let retries = 0;\n",
       "        const open = () => {\n",
       "          if (comm.active) {\n",
       "            comm.open();\n",
       "          } else if (retries > 3) {\n",
       "            console.warn('Comm target never activated')\n",
       "          } else {\n",
       "            retries += 1\n",
       "            setTimeout(open, 500)\n",
       "          }\n",
       "        }\n",
       "        if (comm.active) {\n",
       "          comm.open();\n",
       "        } else {\n",
       "          setTimeout(open, 500)\n",
       "        }\n",
       "        if (msg_handler) {\n",
       "          comm.onMsg = msg_handler;\n",
       "        }\n",
       "      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n",
       "        var comm_promise = google.colab.kernel.comms.open(comm_id)\n",
       "        comm_promise.then((comm) => {\n",
       "          window.PyViz.comms[comm_id] = comm;\n",
       "          if (msg_handler) {\n",
       "            var messages = comm.messages[Symbol.asyncIterator]();\n",
       "            function processIteratorResult(result) {\n",
       "              var message = result.value;\n",
       "              var content = {data: message.data};\n",
       "              var metadata = message.metadata || {comm_id};\n",
       "              var msg = {content, metadata}\n",
       "              msg_handler(msg);\n",
       "              return messages.next().then(processIteratorResult);\n",
       "            }\n",
       "            return messages.next().then(processIteratorResult);\n",
       "          }\n",
       "        })\n",
       "        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n",
       "          return comm_promise.then((comm) => {\n",
       "            comm.send(data, metadata, buffers, disposeOnDone);\n",
       "          });\n",
       "        };\n",
       "        var comm = {\n",
       "          send: sendClosure\n",
       "        };\n",
       "      }\n",
       "      window.PyViz.comms[comm_id] = comm;\n",
       "      return comm;\n",
       "    }\n",
       "    window.PyViz.comm_manager = new JupyterCommManager();\n",
       "    \n",
       "\n",
       "\n",
       "var JS_MIME_TYPE = 'application/javascript';\n",
       "var HTML_MIME_TYPE = 'text/html';\n",
       "var EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\n",
       "var CLASS_NAME = 'output';\n",
       "\n",
       "/**\n",
       " * Render data to the DOM node\n",
       " */\n",
       "function render(props, node) {\n",
       "  var div = document.createElement(\"div\");\n",
       "  var script = document.createElement(\"script\");\n",
       "  node.appendChild(div);\n",
       "  node.appendChild(script);\n",
       "}\n",
       "\n",
       "/**\n",
       " * Handle when a new output is added\n",
       " */\n",
       "function handle_add_output(event, handle) {\n",
       "  var output_area = handle.output_area;\n",
       "  var output = handle.output;\n",
       "  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "    return\n",
       "  }\n",
       "  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "  if (id !== undefined) {\n",
       "    var nchildren = toinsert.length;\n",
       "    var html_node = toinsert[nchildren-1].children[0];\n",
       "    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "    var scripts = [];\n",
       "    var nodelist = html_node.querySelectorAll(\"script\");\n",
       "    for (var i in nodelist) {\n",
       "      if (nodelist.hasOwnProperty(i)) {\n",
       "        scripts.push(nodelist[i])\n",
       "      }\n",
       "    }\n",
       "\n",
       "    scripts.forEach( function (oldScript) {\n",
       "      var newScript = document.createElement(\"script\");\n",
       "      var attrs = [];\n",
       "      var nodemap = oldScript.attributes;\n",
       "      for (var j in nodemap) {\n",
       "        if (nodemap.hasOwnProperty(j)) {\n",
       "          attrs.push(nodemap[j])\n",
       "        }\n",
       "      }\n",
       "      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n",
       "      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n",
       "      oldScript.parentNode.replaceChild(newScript, oldScript);\n",
       "    });\n",
       "    if (JS_MIME_TYPE in output.data) {\n",
       "      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n",
       "    }\n",
       "    output_area._hv_plot_id = id;\n",
       "    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n",
       "      window.PyViz.plot_index[id] = Bokeh.index[id];\n",
       "    } else {\n",
       "      window.PyViz.plot_index[id] = null;\n",
       "    }\n",
       "  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "    var bk_div = document.createElement(\"div\");\n",
       "    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "    var script_attrs = bk_div.children[0].attributes;\n",
       "    for (var i = 0; i < script_attrs.length; i++) {\n",
       "      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "    }\n",
       "    // store reference to server id on output_area\n",
       "    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "  }\n",
       "}\n",
       "\n",
       "/**\n",
       " * Handle when an output is cleared or removed\n",
       " */\n",
       "function handle_clear_output(event, handle) {\n",
       "  var id = handle.cell.output_area._hv_plot_id;\n",
       "  var server_id = handle.cell.output_area._bokeh_server_id;\n",
       "  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n",
       "  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n",
       "  if (server_id !== null) {\n",
       "    comm.send({event_type: 'server_delete', 'id': server_id});\n",
       "    return;\n",
       "  } else if (comm !== null) {\n",
       "    comm.send({event_type: 'delete', 'id': id});\n",
       "  }\n",
       "  delete PyViz.plot_index[id];\n",
       "  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n",
       "    var doc = window.Bokeh.index[id].model.document\n",
       "    doc.clear();\n",
       "    const i = window.Bokeh.documents.indexOf(doc);\n",
       "    if (i > -1) {\n",
       "      window.Bokeh.documents.splice(i, 1);\n",
       "    }\n",
       "  }\n",
       "}\n",
       "\n",
       "/**\n",
       " * Handle kernel restart event\n",
       " */\n",
       "function handle_kernel_cleanup(event, handle) {\n",
       "  delete PyViz.comms[\"hv-extension-comm\"];\n",
       "  window.PyViz.plot_index = {}\n",
       "}\n",
       "\n",
       "/**\n",
       " * Handle update_display_data messages\n",
       " */\n",
       "function handle_update_output(event, handle) {\n",
       "  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n",
       "  handle_add_output(event, handle)\n",
       "}\n",
       "\n",
       "function register_renderer(events, OutputArea) {\n",
       "  function append_mime(data, metadata, element) {\n",
       "    // create a DOM node to render to\n",
       "    var toinsert = this.create_output_subarea(\n",
       "    metadata,\n",
       "    CLASS_NAME,\n",
       "    EXEC_MIME_TYPE\n",
       "    );\n",
       "    this.keyboard_manager.register_events(toinsert);\n",
       "    // Render to node\n",
       "    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "    render(props, toinsert[0]);\n",
       "    element.append(toinsert);\n",
       "    return toinsert\n",
       "  }\n",
       "\n",
       "  events.on('output_added.OutputArea', handle_add_output);\n",
       "  events.on('output_updated.OutputArea', handle_update_output);\n",
       "  events.on('clear_output.CodeCell', handle_clear_output);\n",
       "  events.on('delete.Cell', handle_clear_output);\n",
       "  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n",
       "\n",
       "  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "    safe: true,\n",
       "    index: 0\n",
       "  });\n",
       "}\n",
       "\n",
       "if (window.Jupyter !== undefined) {\n",
       "  try {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  } catch(err) {\n",
       "  }\n",
       "}\n"
      ],
      "application/vnd.holoviews_load.v0+json": "\nif ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n}\n\n\n    function JupyterCommManager() {\n    }\n\n    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        comm_manager.register_target(comm_id, function(comm) {\n          comm.on_msg(msg_handler);\n        });\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n          comm.onMsg = msg_handler;\n        });\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n          var messages = comm.messages[Symbol.asyncIterator]();\n          function processIteratorResult(result) {\n            var message = result.value;\n            var content = {data: message.data, comm_id};\n            var buffers = []\n            for (var buffer of message.buffers || []) {\n              buffers.push(new DataView(buffer))\n            }\n            var metadata = message.metadata || {};\n            var msg = {content, buffers, metadata}\n            msg_handler(msg);\n            return messages.next().then(processIteratorResult);\n          }\n          return messages.next().then(processIteratorResult);\n        })\n      }\n    }\n\n    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n      if (comm_id in window.PyViz.comms) {\n        return window.PyViz.comms[comm_id];\n      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n        if (msg_handler) {\n          comm.on_msg(msg_handler);\n        }\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n        let retries = 0;\n        const open = () => {\n          if (comm.active) {\n            comm.open();\n          } else if (retries > 3) {\n            console.warn('Comm target never activated')\n          } else {\n            retries += 1\n            setTimeout(open, 500)\n          }\n        }\n        if (comm.active) {\n          comm.open();\n        } else {\n          setTimeout(open, 500)\n        }\n        if (msg_handler) {\n          comm.onMsg = msg_handler;\n        }\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        var comm_promise = google.colab.kernel.comms.open(comm_id)\n        comm_promise.then((comm) => {\n          window.PyViz.comms[comm_id] = comm;\n          if (msg_handler) {\n            var messages = comm.messages[Symbol.asyncIterator]();\n            function processIteratorResult(result) {\n              var message = result.value;\n              var content = {data: message.data};\n              var metadata = message.metadata || {comm_id};\n              var msg = {content, metadata}\n              msg_handler(msg);\n              return messages.next().then(processIteratorResult);\n            }\n            return messages.next().then(processIteratorResult);\n          }\n        })\n        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n          return comm_promise.then((comm) => {\n            comm.send(data, metadata, buffers, disposeOnDone);\n          });\n        };\n        var comm = {\n          send: sendClosure\n        };\n      }\n      window.PyViz.comms[comm_id] = comm;\n      return comm;\n    }\n    window.PyViz.comm_manager = new JupyterCommManager();\n    \n\n\nvar JS_MIME_TYPE = 'application/javascript';\nvar HTML_MIME_TYPE = 'text/html';\nvar EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\nvar CLASS_NAME = 'output';\n\n/**\n * Render data to the DOM node\n */\nfunction render(props, node) {\n  var div = document.createElement(\"div\");\n  var script = document.createElement(\"script\");\n  node.appendChild(div);\n  node.appendChild(script);\n}\n\n/**\n * Handle when a new output is added\n */\nfunction handle_add_output(event, handle) {\n  var output_area = handle.output_area;\n  var output = handle.output;\n  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n    return\n  }\n  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n  if (id !== undefined) {\n    var nchildren = toinsert.length;\n    var html_node = toinsert[nchildren-1].children[0];\n    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n    var scripts = [];\n    var nodelist = html_node.querySelectorAll(\"script\");\n    for (var i in nodelist) {\n      if (nodelist.hasOwnProperty(i)) {\n        scripts.push(nodelist[i])\n      }\n    }\n\n    scripts.forEach( function (oldScript) {\n      var newScript = document.createElement(\"script\");\n      var attrs = [];\n      var nodemap = oldScript.attributes;\n      for (var j in nodemap) {\n        if (nodemap.hasOwnProperty(j)) {\n          attrs.push(nodemap[j])\n        }\n      }\n      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n      oldScript.parentNode.replaceChild(newScript, oldScript);\n    });\n    if (JS_MIME_TYPE in output.data) {\n      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n    }\n    output_area._hv_plot_id = id;\n    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n      window.PyViz.plot_index[id] = Bokeh.index[id];\n    } else {\n      window.PyViz.plot_index[id] = null;\n    }\n  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n    var bk_div = document.createElement(\"div\");\n    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n    var script_attrs = bk_div.children[0].attributes;\n    for (var i = 0; i < script_attrs.length; i++) {\n      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n    }\n    // store reference to server id on output_area\n    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n  }\n}\n\n/**\n * Handle when an output is cleared or removed\n */\nfunction handle_clear_output(event, handle) {\n  var id = handle.cell.output_area._hv_plot_id;\n  var server_id = handle.cell.output_area._bokeh_server_id;\n  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n  if (server_id !== null) {\n    comm.send({event_type: 'server_delete', 'id': server_id});\n    return;\n  } else if (comm !== null) {\n    comm.send({event_type: 'delete', 'id': id});\n  }\n  delete PyViz.plot_index[id];\n  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n    var doc = window.Bokeh.index[id].model.document\n    doc.clear();\n    const i = window.Bokeh.documents.indexOf(doc);\n    if (i > -1) {\n      window.Bokeh.documents.splice(i, 1);\n    }\n  }\n}\n\n/**\n * Handle kernel restart event\n */\nfunction handle_kernel_cleanup(event, handle) {\n  delete PyViz.comms[\"hv-extension-comm\"];\n  window.PyViz.plot_index = {}\n}\n\n/**\n * Handle update_display_data messages\n */\nfunction handle_update_output(event, handle) {\n  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n  handle_add_output(event, handle)\n}\n\nfunction register_renderer(events, OutputArea) {\n  function append_mime(data, metadata, element) {\n    // create a DOM node to render to\n    var toinsert = this.create_output_subarea(\n    metadata,\n    CLASS_NAME,\n    EXEC_MIME_TYPE\n    );\n    this.keyboard_manager.register_events(toinsert);\n    // Render to node\n    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n    render(props, toinsert[0]);\n    element.append(toinsert);\n    return toinsert\n  }\n\n  events.on('output_added.OutputArea', handle_add_output);\n  events.on('output_updated.OutputArea', handle_update_output);\n  events.on('clear_output.CodeCell', handle_clear_output);\n  events.on('delete.Cell', handle_clear_output);\n  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n\n  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n    safe: true,\n    index: 0\n  });\n}\n\nif (window.Jupyter !== undefined) {\n  try {\n    var events = require('base/js/events');\n    var OutputArea = require('notebook/js/outputarea').OutputArea;\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  } catch(err) {\n  }\n}\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.holoviews_exec.v0+json": "",
      "text/html": [
       "<div id='e269707f-1b40-4cf0-a49a-6f2db563fd9b'>\n",
       "  <div id=\"d30f26f8-a3fa-46be-b05b-9fa6621dab42\" data-root-id=\"e269707f-1b40-4cf0-a49a-6f2db563fd9b\" style=\"display: contents;\"></div>\n",
       "</div>\n",
       "<script type=\"application/javascript\">(function(root) {\n",
       "  var docs_json = {\"887bc1df-4534-45dc-afad-4f6bad02e7bc\":{\"version\":\"3.7.3\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"panel.models.browser.BrowserInfo\",\"id\":\"e269707f-1b40-4cf0-a49a-6f2db563fd9b\"},{\"type\":\"object\",\"name\":\"panel.models.comm_manager.CommManager\",\"id\":\"587b160a-46f5-451d-b0d1-93ff8afe8fb1\",\"attributes\":{\"plot_id\":\"e269707f-1b40-4cf0-a49a-6f2db563fd9b\",\"comm_id\":\"fcaff48dc8a2414db6e58ed9f03d5e24\",\"client_comm_id\":\"b392eafe38bb44f29250b595d054a03e\"}}],\"defs\":[{\"type\":\"model\",\"name\":\"ReactiveHTML1\"},{\"type\":\"model\",\"name\":\"FlexBox1\",\"properties\":[{\"name\":\"align_content\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"align_items\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"flex_direction\",\"kind\":\"Any\",\"default\":\"row\"},{\"name\":\"flex_wrap\",\"kind\":\"Any\",\"default\":\"wrap\"},{\"name\":\"gap\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"justify_content\",\"kind\":\"Any\",\"default\":\"flex-start\"}]},{\"type\":\"model\",\"name\":\"FloatPanel1\",\"properties\":[{\"name\":\"config\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"contained\",\"kind\":\"Any\",\"default\":true},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"right-top\"},{\"name\":\"offsetx\",\"kind\":\"Any\",\"default\":null},{\"name\":\"offsety\",\"kind\":\"Any\",\"default\":null},{\"name\":\"theme\",\"kind\":\"Any\",\"default\":\"primary\"},{\"name\":\"status\",\"kind\":\"Any\",\"default\":\"normalized\"}]},{\"type\":\"model\",\"name\":\"GridStack1\",\"properties\":[{\"name\":\"ncols\",\"kind\":\"Any\",\"default\":null},{\"name\":\"nrows\",\"kind\":\"Any\",\"default\":null},{\"name\":\"allow_resize\",\"kind\":\"Any\",\"default\":true},{\"name\":\"allow_drag\",\"kind\":\"Any\",\"default\":true},{\"name\":\"state\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"drag1\",\"properties\":[{\"name\":\"slider_width\",\"kind\":\"Any\",\"default\":5},{\"name\":\"slider_color\",\"kind\":\"Any\",\"default\":\"black\"},{\"name\":\"value\",\"kind\":\"Any\",\"default\":50}]},{\"type\":\"model\",\"name\":\"click1\",\"properties\":[{\"name\":\"terminal_output\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"debug_name\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"clears\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"ReactiveESM1\",\"properties\":[{\"name\":\"esm_constants\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}}]},{\"type\":\"model\",\"name\":\"JSComponent1\",\"properties\":[{\"name\":\"esm_constants\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}}]},{\"type\":\"model\",\"name\":\"ReactComponent1\",\"properties\":[{\"name\":\"use_shadow_dom\",\"kind\":\"Any\",\"default\":true},{\"name\":\"esm_constants\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}}]},{\"type\":\"model\",\"name\":\"AnyWidgetComponent1\",\"properties\":[{\"name\":\"use_shadow_dom\",\"kind\":\"Any\",\"default\":true},{\"name\":\"esm_constants\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}}]},{\"type\":\"model\",\"name\":\"FastWrapper1\",\"properties\":[{\"name\":\"object\",\"kind\":\"Any\",\"default\":null},{\"name\":\"style\",\"kind\":\"Any\",\"default\":null}]},{\"type\":\"model\",\"name\":\"NotificationArea1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"max_notifications\",\"kind\":\"Any\",\"default\":5},{\"name\":\"notifications\",\"kind\":\"Any\",\"default\":[]},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0},{\"name\":\"types\",\"kind\":\"Any\",\"default\":[{\"type\":\"map\",\"entries\":[[\"type\",\"warning\"],[\"background\",\"#ffc107\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-exclamation-triangle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]},{\"type\":\"map\",\"entries\":[[\"type\",\"info\"],[\"background\",\"#007bff\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-info-circle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]}]}]},{\"type\":\"model\",\"name\":\"Notification\",\"properties\":[{\"name\":\"background\",\"kind\":\"Any\",\"default\":null},{\"name\":\"duration\",\"kind\":\"Any\",\"default\":3000},{\"name\":\"icon\",\"kind\":\"Any\",\"default\":null},{\"name\":\"message\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"notification_type\",\"kind\":\"Any\",\"default\":null},{\"name\":\"_rendered\",\"kind\":\"Any\",\"default\":false},{\"name\":\"_destroyed\",\"kind\":\"Any\",\"default\":false}]},{\"type\":\"model\",\"name\":\"TemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"BootstrapTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"TemplateEditor1\",\"properties\":[{\"name\":\"layout\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"MaterialTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"request_value1\",\"properties\":[{\"name\":\"fill\",\"kind\":\"Any\",\"default\":\"none\"},{\"name\":\"_synced\",\"kind\":\"Any\",\"default\":null},{\"name\":\"_request_sync\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"holoviews.plotting.bokeh.raster.HoverModel\",\"properties\":[{\"name\":\"xy\",\"kind\":\"Any\",\"default\":null},{\"name\":\"data\",\"kind\":\"Any\",\"default\":null}]}]}};\n",
       "  var render_items = [{\"docid\":\"887bc1df-4534-45dc-afad-4f6bad02e7bc\",\"roots\":{\"e269707f-1b40-4cf0-a49a-6f2db563fd9b\":\"d30f26f8-a3fa-46be-b05b-9fa6621dab42\"},\"root_ids\":[\"e269707f-1b40-4cf0-a49a-6f2db563fd9b\"]}];\n",
       "  var docs = Object.values(docs_json)\n",
       "  if (!docs) {\n",
       "    return\n",
       "  }\n",
       "  const py_version = docs[0].version.replace('rc', '-rc.').replace('.dev', '-dev.')\n",
       "  async function embed_document(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    await Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "    for (const render_item of render_items) {\n",
       "      for (const root_id of render_item.root_ids) {\n",
       "\tconst id_el = document.getElementById(root_id)\n",
       "\tif (id_el.children.length && id_el.children[0].hasAttribute('data-root-id')) {\n",
       "\t  const root_el = id_el.children[0]\n",
       "\t  root_el.id = root_el.id + '-rendered'\n",
       "\t  for (const child of root_el.children) {\n",
       "            // Ensure JupyterLab does not capture keyboard shortcuts\n",
       "            // see: https://jupyterlab.readthedocs.io/en/4.1.x/extension/notebook.html#keyboard-interaction-model\n",
       "\t    child.setAttribute('data-lm-suppress-shortcuts', 'true')\n",
       "\t  }\n",
       "\t}\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  function get_bokeh(root) {\n",
       "    if (root.Bokeh === undefined) {\n",
       "      return null\n",
       "    } else if (root.Bokeh.version !== py_version) {\n",
       "      if (root.Bokeh.versions === undefined || !root.Bokeh.versions.has(py_version)) {\n",
       "\treturn null\n",
       "      }\n",
       "      return root.Bokeh.versions.get(py_version);\n",
       "    } else if (root.Bokeh.version === py_version) {\n",
       "      return root.Bokeh\n",
       "    }\n",
       "    return null\n",
       "  }\n",
       "  function is_loaded(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    return (Bokeh != null && Bokeh.Panel !== undefined)\n",
       "  }\n",
       "  if (is_loaded(root)) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (is_loaded(root)) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else if (document.readyState == \"complete\") {\n",
       "        attempts++;\n",
       "        if (attempts > 200) {\n",
       "          clearInterval(timer);\n",
       "\t  var Bokeh = get_bokeh(root)\n",
       "\t  if (Bokeh == null || Bokeh.Panel == null) {\n",
       "            console.warn(\"Panel: ERROR: Unable to run Panel code because Bokeh or Panel library is missing\");\n",
       "\t  } else {\n",
       "\t    console.warn(\"Panel: WARNING: Attempting to render but not all required libraries could be resolved.\")\n",
       "\t    embed_document(root)\n",
       "\t  }\n",
       "        }\n",
       "      }\n",
       "    }, 25, root)\n",
       "  }\n",
       "})(window);</script>"
      ]
     },
     "metadata": {
      "application/vnd.holoviews_exec.v0+json": {
       "id": "e269707f-1b40-4cf0-a49a-6f2db563fd9b"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "#from skimage import exposure\n",
    "#from skimage.io import imsave, imread\n",
    "#from osgeo import ogr\n",
    "import pystac_client\n",
    "from pyproj import Transformer\n",
    "from datetime import date, timedelta, datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import geoviews as gv\n",
    "import hvplot.pandas\n",
    "import intake\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "import intake\n",
    "from pyproj import Proj, transform\n",
    "#from osgeo import gdal\n",
    "from sklearn.neighbors import BallTree\n",
    "import earthaccess\n",
    "import gzip\n",
    "import s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3581d67-e877-45f7-9391-b7ecab5ca31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for progress bar\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "from ipywidgets import interact, Dropdown\n",
    "import time\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "import boto3\n",
    "import rasterio as rio\n",
    "from rasterio.features import rasterize\n",
    "from rasterio.session import AWSSession\n",
    "import dask\n",
    "import os\n",
    "import rioxarray\n",
    "from rasterio.enums import Resampling\n",
    "from rasterio.warp import reproject\n",
    "from rasterio.warp import Resampling as resample\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy\n",
    "#from pykrige.ok import OrdinaryKriging\n",
    "from sklearn.linear_model import LinearRegression, RANSACRegressor\n",
    "from scipy.odr import Model, RealData, ODR\n",
    "import scipy.odr as odr\n",
    "import scipy\n",
    "import statsmodels.formula.api as smf\n",
    "from shapely.geometry.polygon import Polygon, Point\n",
    "import pygmt\n",
    "import gc\n",
    "import pytz\n",
    "import pyproj\n",
    "import math\n",
    "from pathlib import Path\n",
    "from matplotlib.patches import Polygon as Pgon\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import SSTutils as sut\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0eae27-4580-434a-a934-6a5c80821a11",
   "metadata": {},
   "source": [
    "## Why do we need to validate?\n",
    "Now we have our calibrated SST data product!  We're almost ready to use this data product for our scientific analysis, but there is still one more important step. As a quick reminder, our data product was derived from top-of-atmosphere radiance measurements. We then converted these radiances into actual SST values (in units of temperature) by calibrating with another satellite (MODIS). But can we trust that these derived SST values are accurately reflecting real ocean surface temperatures? Validation allows us to quantitfy the uncertainty of our product."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90135ee1-bcab-4806-a475-eaa10e4cf2e8",
   "metadata": {},
   "source": [
    "## Finding data to validate with\n",
    "When validating satellite products, we typically want to use in situ measurements: i.e., direct measurements taken in the field. For the ocean, these measurements come from buoys, ship-based thermometers and autonomous floats. An important thing to keep in mind is that while satellites measure the skin temperature (top ~1020 microns), in situ platforms measure bulk SST (a few cm to 1 m depth). \n",
    "\n",
    "Should be high quality!\n",
    "\n",
    "Finding a good validation dataset can be tricky. Here's are some lists of commonly-used validation datasets to help get you started: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb65da60-7ccd-45a1-b46e-84caa36a5d29",
   "metadata": {},
   "source": [
    "TO DO: These links need to be checked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a455fd1c-c2cd-4146-88c6-955f470bb9aa",
   "metadata": {},
   "source": [
    "```{admonition}  Ocean \n",
    ":class: toggle\n",
    "    \n",
    "| **Dataset**        | **Access**                                                                                                                                                                                                                                                                                                   | **Variables**                     |\n",
    "| ------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | --------------------------------- |\n",
    "| **Argo**           | [argo.ucsd.edu](https://argo.ucsd.edu/data/acknowledging-argo/)   DOI: [10.17882/42182](https://doi.org/10.17882/42182) <br> [NOAA archive](https://www.ncei.noaa.gov/access/metadata/landing-page/bin/iso?id=gov.noaa.nodc%3AArgo-Monthly)  DOI: [10.25921/q97ed719](https://doi.org/10.25921/q97e-d719) | Sea surface temperature, salinity |\n",
    "| **GHRSST iQuam**   | [GHRSST iQuam website](https://www.star.nesdis.noaa.gov/socd/sst/iquam/) <br> [GHRSST documentation (Zenodo)](https://doi.org/10.5281/zenodo.7589540)                                                                                                                                                        | Sea surface temperature           |\n",
    "| **GTSPP**          | [NCEI GTSPP access](https://www.ncei.noaa.gov/products/global-temperature-salinity-profile-program)                                                                                                                                                                                                          | Temperature, salinity profiles    |\n",
    "| **SPURS**          | [SPURS homepage (NASA)](https://spurs.jpl.nasa.gov/)                                                                                                                                                                                                                                                         | Sea surface salinity              |\n",
    "| **SeaBASS**        | [seabass.gsfc.nasa.gov](https://seabass.gsfc.nasa.gov/)                                                                                                                                                                                                                                                      | Ocean color, chlorophyll-a        |\n",
    "| **NOMAD**          | [NOMAD at NOAA STAR](https://www.star.nesdis.noaa.gov/socd/ocean/color/NOMAD/NOMAD.shtml)                                                                                                                                                                                                                    | Ocean color, chlorophyll-a        |\n",
    "| **BOUSSOLE**       | [BOUSSOLE Project](https://www.obs-vlfr.fr/Boussole/html/home/home.php)                                                                                                                                                                                                                                      | Ocean color, optics, chlorophyll  |\n",
    "| **GDP (Drifters)** | [Global Drifter Program](https://www.aoml.noaa.gov/phod/gdp/)                                                                                                                                                                                                                                                | Ocean surface currents            |\n",
    "| **HF Radar**       | [HFRNet Portal](https://hfrnet.ucsd.edu/)                                                                                                                                                                                                                                                                    | Ocean surface currents            |\n",
    "| **ADCP**           | [NOAA ADCP Program](https://www.aoml.noaa.gov/phod/goos/adcp/index.php)                                                                                                                                                                                                                                      | Water column currents             |\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f40760-4d49-47f4-a6cc-f8d801be0b09",
   "metadata": {},
   "source": [
    "```{admonition}  Atmosphere \n",
    ":class: toggle\n",
    "    \n",
    "| **Dataset**    | **Access**                                                                                                                    | **Variables**                             |\n",
    "| -------------- | ----------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------- |\n",
    "| **AERONET**    | [aeronet.gsfc.nasa.gov](https://aeronet.gsfc.nasa.gov/) <br> [re3data entry](https://www.re3data.org/repository/r3d100011742) | Aerosol optical depth (AOD), aerosol type |\n",
    "| **IGRA**       | [IGRA at NOAA](https://www.ncei.noaa.gov/products/integrated-global-radiosonde-archive)                                       | Atmospheric temperature, pressure         |\n",
    "| **GRUAN**      | [GRUAN](https://www.gruan.org/)                                                                                               | Temperature, humidity, pressure           |\n",
    "| **METAR/ASOS** | [Iowa State Mesonet](https://mesonet.agron.iastate.edu/request/download.phtml)                                                | Surface temperature, pressure             |\n",
    "| **GPM-GV**     | [NASA GPM Ground Validation](https://gpm.nasa.gov/resources/ground-validation)                                                | Precipitation                             |\n",
    "| **GPCC**       | [GPCC at DWD](https://opendata.dwd.de/climate_environment/GPCC/)                                                              | Precipitation                             |\n",
    "| **SKYNET**     | [Chiba University SKYNET](http://atmos.cr.chiba-u.ac.jp/skynet/)                                                              | Aerosols, AOD                             |\n",
    "| **MPLNET**     | [MPLNET at NASA](https://mplnet.gsfc.nasa.gov/)                                                                               | Aerosols, clouds                          |\n",
    "| **TCCON**      | [TCCON Data Portal](https://tccondata.org/)                                                                                   | CO, CH, other trace gases               |\n",
    "| **Pandora**    | [Pandora Project](https://pandora.gsfc.nasa.gov/)                                                                             | NO, O, trace gases                      |\n",
    "| **MAX-DOAS**   | [MAX-DOAS Network](https://uv-vis.aeronomie.be/maxdoas/)                                                                      | NO, SO, HCHO                            |\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0744b29-1497-4384-8a53-80428b21268e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "```{admonition}  Land\n",
    ":class: toggle\n",
    "\n",
    "| **Dataset**  | **Access**                                                           | **Variables**                          |\n",
    "| ------------ | -------------------------------------------------------------------- | -------------------------------------- |\n",
    "| **SURFRAD**  | [NOAA SURFRAD](https://www.esrl.noaa.gov/gmd/grad/surfrad/)          | Land surface temperature, radiation    |\n",
    "| **BSRN**     | [BSRN](https://bsrn.awi.de/)                                         | Solar radiation, surface energy fluxes |\n",
    "| **FLUXNET**  | [fluxnet.org](https://fluxnet.org/)                                  | LST, vegetation indices, fluxes        |\n",
    "| **SCAN**     | [USDA SCAN](https://www.wcc.nrcs.usda.gov/scan/)                     | Soil moisture                          |\n",
    "| **ISMN**     | [International Soil Moisture Network](https://ismn.earth/)           | Soil moisture                          |\n",
    "| **Phenocam** | [Phenocam Network](https://phenocam.sr.unh.edu/)                     | Vegetation phenology (NDVI proxy)      |\n",
    "| **NEON**     | [NEON Data Portal](https://www.neonscience.org/)                     | Vegetation indices, climate variables  |\n",
    "| **BELMANIP** | [Copernicus Land Service](https://land.copernicus.eu/global/sites)   | Leaf area index (LAI)                  |\n",
    "| **VALERI**   | [VALERI Project](https://w3.avignon.inrae.fr/valeri/)                | LAI, fAPAR                             |\n",
    "| **DIRECT**   | [VALERI/DIRECT Info](https://www.avignon.inrae.fr/valeri/?q=node/13) | LAI                                    |\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f9b733-3643-4599-9934-b7c9c3f15261",
   "metadata": {},
   "source": [
    "```{admonition}  Cryosphere \n",
    ":class: toggle\n",
    " \n",
    "| **Dataset**          | **Access**                                                                                                       | **Variables**                        |\n",
    "| -------------------- | ---------------------------------------------------------------------------------------------------------------- | ------------------------------------ |\n",
    "| **SnowEx**           | [NASA SnowEx](https://snow.nasa.gov/campaigns/snowex)                                                            | Snow depth, snow cover, SWE          |\n",
    "| **CALVAL (NSIDC)**   | [NSIDC Cal/Val](https://nsidc.org/data/calval)                                                                   | Snow, cryosphere                     |\n",
    "| **GSNOW**            | [NOAA GSNOW](https://www.ncei.noaa.gov/products/snow-cover)                                                      | Snow cover                           |\n",
    "| **NSIDC**            | [NSIDC](https://nsidc.org/)                                                                                      | Sea ice, snow cover                  |\n",
    "| **IABP (Ice Buoys)** | [International Arctic Buoy Program](https://iabp.apl.uw.edu/)                                                    | Sea ice concentration, drift         |\n",
    "| **IceBridge**        | [NASA IceBridge](https://nsidc.org/data/icebridge)                                                               | Ice sheet elevation, thickness       |\n",
    "| **ATM**              | [Airborne Topographic Mapper (ATM)](https://nsidc.org/data/ILATM2)                                               | Ice elevation profiles               |\n",
    "| **NOHRSC**           | [NOAA NOHRSC](https://www.nohrsc.noaa.gov/)                                                                      | Snow depth and snow water equivalent |\n",
    "| **CCSN (Canada)**    | [Canadian Cryospheric Snow Network](https://open.canada.ca/data/en/dataset/5ff9d50e-591f-4e06-9b8f-0a707e8a1a74) | Snow depth, SWE                      |\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8e68c2-c011-4263-aa53-dc2317ebe57f",
   "metadata": {},
   "source": [
    "### Example: GHRSST iQUAM data\n",
    "For our new SST product, we'll use the GHRSST iQUAM dataset for validation. GHRSST is the Group for High Resolution Sea Surface Temperature  an international collaboration supporting high-quality SST products for research and operational use  and iQuam is a system developed by NOAA to collect, quality-control (QC), and distribute in situ SST observations in near real-time and delayed mode. This dataset aggregates SST measurements from a variety of in situ platforms, such as drifting buoys, moored buoys, shipboard sensors, and Argo floats. \n",
    "\n",
    "https://www.star.nesdis.noaa.gov/socd/sst/iquam/?tab=0&dateinput_year=2023&dateinput_month=02&dayofmoninput_day=26&dateinput_hour=00&dayofmon=monthly&qcrefsst=_qcrey&qcrefsst=_qccmc&outlier=qced#qmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5bf864-229d-4cbf-bf06-796673d747dd",
   "metadata": {},
   "source": [
    "## Finding data matchups"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1a183ab8-b51c-46dc-993b-6d7feaed159d",
   "metadata": {},
   "source": [
    "Here are some important considerations we need to make when finding matchups between the satellite product and in situ datasets:\n",
    "\n",
    " **1. Spatial Collocation**\n",
    "- Footprint Differences: Satellite pixels often represent an area (e.g., 1 km or more), while in situ data may be point measurements.\n",
    "    - Solution: Use in situ data averaged over the satellite footprint or compare satellite data averaged over multiple pixels surrounding the in situ point.\n",
    "\n",
    "- Geolocation Accuracy: Both satellite and in situ positions should be accurately known, especially in dynamic environments (e.g., drifting buoys).\n",
    "\n",
    "- Environmental Variability: Spatial gradients (e.g., near coastlines or fronts) can lead to mismatches even with close locations.\n",
    "\n",
    " **2. Temporal Matching**\n",
    "- Temporal Resolution: Satellites provide snapshots (sometimes daily, sometimes instantaneous), while in situ data may be continuous or periodic.\n",
    "    - Solution: Match satellite observation time as closely as possible to in situ sampling time (e.g., within 1 hour).\n",
    "\n",
    "- Diurnal Effects: Some variables (e.g., SST, radiative fluxes) vary significantly throughout the day.\n",
    "    - Solution: Use diurnal correction or only match at known overpass times (e.g., MODIS ~1:30 pm local time).\n",
    "\n",
    " **3. Variable Definitions and Depths**\n",
    "- Depth Differences: For example, satellite SST represents skin temperature (top ~1020 m), whereas in situ sensors often measure at ~1 m.\n",
    "    - Solution: Apply a skin-to-bulk correction or compare to foundation temperature from models.\n",
    "- Variable Representation: Ensure the in situ measurement is of the same quantity the satellite estimates (e.g., top-of-canopy reflectance vs. leaf-level LAI).\n",
    "\n",
    " **4. Quality Control**\n",
    "- Flagging: Use only high-quality satellite and in situ data (e.g., use quality flags to exclude cloud-contaminated pixels or questionable sensor readings).\n",
    "- Consistency in Units and Calibration: Verify that data are in the same units and reference systems (e.g., radiance vs. reflectance, SI units).\n",
    "- Error Estimates: Consider measurement uncertainty and noise in both datasets.\n",
    "\n",
    " **5. Statistical Considerations**\n",
    "- Matchup Volume: A large number of matchup pairs increases robustness.\n",
    "- Bias and RMSE Analysis: Use metrics like bias, RMSE, and correlation to assess agreement.\n",
    "- Outlier Handling: Identify and analyze outliers to understand limitations or failure modes.\n",
    "\n",
    " **6. Sensor Calibration**\n",
    "- Ensure both satellite and in situ instruments are properly calibrated and traceable to standards.\n",
    "\n",
    " **7. Geophysical Context**\n",
    "- Geographical Diversity: Validation should include diverse regions (e.g., open ocean, coastal, tropical, polar) to capture algorithm performance globally. In our example, we are only validating our product near Antarctica, so we should only use this product in that region. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b458c9d-2f4a-485d-905e-ef2ababfff98",
   "metadata": {},
   "source": [
    "In order to maximize the number of data matchups while still making a fair comparison, we need to determine an appropriate window in space and time. In our example, we set our spatial window to 1 km and our temporal window to half of a day:\n",
    "```\n",
    "dist = 1.0\n",
    "time_add = 0.5\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f0c860a-e1cd-4baa-9b90-e5ba6b6e1a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/landsatproduct-cookbook/notebooks/Data\n",
      "/home/jovyan/landsatproduct-cookbook/notebooks/Data/iQuam\n"
     ]
    }
   ],
   "source": [
    "# Landsat STAC catalog location\n",
    "url = 'https://landsatlook.usgs.gov/stac-server'\n",
    "\n",
    "# Add Jetstream2 access\n",
    "jetstream_url = 'https://js2.jetstream-cloud.org:8001/'\n",
    "s3 = s3fs.S3FileSystem(anon=True, client_kwargs=dict(endpoint_url=jetstream_url))\n",
    "iQfiles=s3.ls('pythia/landsat8/iQuam') \n",
    "#print(iQfiles)\n",
    "\n",
    "# Paths\n",
    "# note, only the iQuam path will run correctly\n",
    "basepath = Path('/home/jovyan/landsatproduct-cookbook')\n",
    "lsatpath = basepath / 'Data'\n",
    "atmpath = lsatpath / 'AtmCorrection'\n",
    "modout_path = lsatpath / 'MOD07_L2'\n",
    "SSTpath = lsatpath / 'SST/Validation/iQuamIntercomp/'\n",
    "#iQpath = lsatpath / 'iQuam'\n",
    "\n",
    "# Set up the directory structure to hold ERA5 atmospheric profiles and SST data\n",
    "!mkdir Data\n",
    "%cd Data\n",
    "!mkdir iQuam\n",
    "%cd iQuam\n",
    "\n",
    "WV = 'Water_Vapor'\n",
    "\n",
    "# For geopandas and tile plots\n",
    "satellite = 'Landsat8'\n",
    "collection = 'landsat-c2l1' # Landsat Collection 2, Level 1\n",
    "colnm = ['landsat:wrs_path','landsat:wrs_row']\n",
    "gjson_outfile = lsatpath / f'{satellite}_iQuam.geojson'\n",
    "\n",
    "# Buffer around iquam point used to create a bounding box for Landsat sample\n",
    "dist = 1.0 # km\n",
    "\n",
    "# Temporal search range (days) before/after iquam measurement for finding Landsat image\n",
    "time_add = 0.5\n",
    "\n",
    "lthresh = -1.9\n",
    "\n",
    "interp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2527ab54-4b5f-4b74-821e-31a788e0d873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year and months desired (multiple years)\n",
    "start_yr = 2013\n",
    "end_yr = 2023\n",
    "\n",
    "# Note these will get months from the later part of the year to early next\n",
    "start_mo = '09'\n",
    "end_mo = '03'\n",
    "\n",
    "# Headers for the saved outputs\n",
    "headers = ['DateTime','L8_filename','L8_SST','L8_std','center','N','S','E','W','NE','SE','NW','SW','L8_SST_max','L8_SST_min','Argo_id','Argo_SST','A_lat','A_lon']\n",
    "\n",
    "# Desired projection transformation\n",
    "source_crs = 'epsg:4326' \n",
    "target_crs = 'epsg:3031' # Coordinate system of the Landsat file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6a876c-6ad8-4358-b8c5-7059aaf5cfba",
   "metadata": {},
   "source": [
    "Now we're ready to find our matchups!\n",
    "\n",
    "We're repeating the retrieval step for the locations where we have data matchups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6bb9aca9-57cd-40fa-86c1-2d894d2bbc59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013: 0\n",
      "2014: 0\n",
      "2015: 0\n",
      "2016: 0\n",
      "2017: 0\n",
      "2018: 0\n",
      "2019: 0\n",
      "2020: 7\n",
      "pythia/landsat8/iQuam/202103-STAR-L2i_GHRSST-SST-iQuam-V2.10-v01.0-fv06.0.nc\n",
      "s3://pythia/landsat8/iQuam/202103-STAR-L2i_GHRSST-SST-iQuam-V2.10-v01.0-fv06.0.nc\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:44\u001b[0m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/xarray/core/dataset.py:7165\u001b[0m, in \u001b[0;36mDataset.to_dataframe\u001b[0;34m(self, dim_order)\u001b[0m\n\u001b[1;32m   7137\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Convert this dataset into a pandas.DataFrame.\u001b[39;00m\n\u001b[1;32m   7138\u001b[0m \n\u001b[1;32m   7139\u001b[0m \u001b[38;5;124;03mNon-index variables in this dataset form the columns of the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   7160\u001b[0m \n\u001b[1;32m   7161\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   7163\u001b[0m ordered_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_normalize_dim_order(dim_order\u001b[38;5;241m=\u001b[39mdim_order)\n\u001b[0;32m-> 7165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_to_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mordered_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mordered_dims\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/xarray/core/dataset.py:7084\u001b[0m, in \u001b[0;36mDataset._to_dataframe\u001b[0;34m(self, ordered_dims)\u001b[0m\n\u001b[1;32m   7081\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mxarray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextension_array\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PandasExtensionArray\n\u001b[1;32m   7083\u001b[0m columns_in_order \u001b[38;5;241m=\u001b[39m [k \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariables \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdims]\n\u001b[0;32m-> 7084\u001b[0m non_extension_array_columns \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   7085\u001b[0m     k\n\u001b[1;32m   7086\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m columns_in_order\n\u001b[1;32m   7087\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_extension_array_dtype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariables[k]\u001b[38;5;241m.\u001b[39mdata)\n\u001b[1;32m   7088\u001b[0m ]\n\u001b[1;32m   7089\u001b[0m extension_array_columns \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   7090\u001b[0m     k\n\u001b[1;32m   7091\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m columns_in_order\n\u001b[1;32m   7092\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_extension_array_dtype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariables[k]\u001b[38;5;241m.\u001b[39mdata)\n\u001b[1;32m   7093\u001b[0m ]\n\u001b[1;32m   7094\u001b[0m extension_array_columns_different_index \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   7095\u001b[0m     k\n\u001b[1;32m   7096\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m extension_array_columns\n\u001b[1;32m   7097\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariables[k]\u001b[38;5;241m.\u001b[39mdims) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mset\u001b[39m(ordered_dims\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m   7098\u001b[0m ]\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/xarray/core/dataset.py:7087\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   7081\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mxarray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextension_array\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PandasExtensionArray\n\u001b[1;32m   7083\u001b[0m columns_in_order \u001b[38;5;241m=\u001b[39m [k \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariables \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdims]\n\u001b[1;32m   7084\u001b[0m non_extension_array_columns \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   7085\u001b[0m     k\n\u001b[1;32m   7086\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m columns_in_order\n\u001b[0;32m-> 7087\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_extension_array_dtype(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvariables\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m)\n\u001b[1;32m   7088\u001b[0m ]\n\u001b[1;32m   7089\u001b[0m extension_array_columns \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   7090\u001b[0m     k\n\u001b[1;32m   7091\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m columns_in_order\n\u001b[1;32m   7092\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_extension_array_dtype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariables[k]\u001b[38;5;241m.\u001b[39mdata)\n\u001b[1;32m   7093\u001b[0m ]\n\u001b[1;32m   7094\u001b[0m extension_array_columns_different_index \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   7095\u001b[0m     k\n\u001b[1;32m   7096\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m extension_array_columns\n\u001b[1;32m   7097\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariables[k]\u001b[38;5;241m.\u001b[39mdims) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mset\u001b[39m(ordered_dims\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m   7098\u001b[0m ]\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/xarray/core/variable.py:430\u001b[0m, in \u001b[0;36mVariable.data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    428\u001b[0m     duck_array \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data\u001b[38;5;241m.\u001b[39marray\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data, indexing\u001b[38;5;241m.\u001b[39mExplicitlyIndexed):\n\u001b[0;32m--> 430\u001b[0m     duck_array \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_duck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_duck_array(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data):\n\u001b[1;32m    432\u001b[0m     duck_array \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/xarray/core/indexing.py:845\u001b[0m, in \u001b[0;36mMemoryCachedArray.get_duck_array\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_duck_array\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 845\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_cached\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    846\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marray\u001b[38;5;241m.\u001b[39mget_duck_array()\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/xarray/core/indexing.py:842\u001b[0m, in \u001b[0;36mMemoryCachedArray._ensure_cached\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    841\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_ensure_cached\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 842\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marray \u001b[38;5;241m=\u001b[39m as_indexable(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_duck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/xarray/core/indexing.py:799\u001b[0m, in \u001b[0;36mCopyOnWriteArray.get_duck_array\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    798\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_duck_array\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 799\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_duck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/xarray/core/indexing.py:654\u001b[0m, in \u001b[0;36mLazilyIndexedArray.get_duck_array\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    650\u001b[0m     array \u001b[38;5;241m=\u001b[39m apply_indexer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marray, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey)\n\u001b[1;32m    651\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    652\u001b[0m     \u001b[38;5;66;03m# If the array is not an ExplicitlyIndexedNDArrayMixin,\u001b[39;00m\n\u001b[1;32m    653\u001b[0m     \u001b[38;5;66;03m# it may wrap a BackendArray so use its __getitem__\u001b[39;00m\n\u001b[0;32m--> 654\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;66;03m# self.array[self.key] is now a numpy array when\u001b[39;00m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;66;03m# self.array is a BackendArray subclass\u001b[39;00m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;66;03m# and self.key is BasicIndexer((slice(None, None, None),))\u001b[39;00m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;66;03m# so we need the explicit check for ExplicitlyIndexed\u001b[39;00m\n\u001b[1;32m    660\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(array, ExplicitlyIndexed):\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/xarray/backends/h5netcdf_.py:55\u001b[0m, in \u001b[0;36mH5NetCDFArrayWrapper.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[0;32m---> 55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mindexing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplicit_indexing_adapter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIndexingSupport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOUTER_1VECTOR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/xarray/core/indexing.py:1023\u001b[0m, in \u001b[0;36mexplicit_indexing_adapter\u001b[0;34m(key, shape, indexing_support, raw_indexing_method)\u001b[0m\n\u001b[1;32m   1001\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Support explicit indexing by delegating to a raw indexing method.\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m \n\u001b[1;32m   1003\u001b[0m \u001b[38;5;124;03mOuter and/or vectorized indexers are supported by indexing a second time\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;124;03mIndexing result, in the form of a duck numpy-array.\u001b[39;00m\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1022\u001b[0m raw_key, numpy_indices \u001b[38;5;241m=\u001b[39m decompose_indexer(key, shape, indexing_support)\n\u001b[0;32m-> 1023\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mraw_indexing_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_key\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtuple\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m numpy_indices\u001b[38;5;241m.\u001b[39mtuple:\n\u001b[1;32m   1025\u001b[0m     \u001b[38;5;66;03m# index the loaded duck array\u001b[39;00m\n\u001b[1;32m   1026\u001b[0m     indexable \u001b[38;5;241m=\u001b[39m as_indexable(result)\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/xarray/backends/h5netcdf_.py:62\u001b[0m, in \u001b[0;36mH5NetCDFArrayWrapper._getitem\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatastore\u001b[38;5;241m.\u001b[39mlock:\n\u001b[1;32m     61\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_array(needs_lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marray\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/h5netcdf/core.py:535\u001b[0m, in \u001b[0;36mBaseVariable.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    533\u001b[0m     string_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_root\u001b[38;5;241m.\u001b[39m_h5py\u001b[38;5;241m.\u001b[39mcheck_string_dtype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h5ds\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    534\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m string_info \u001b[38;5;129;01mand\u001b[39;00m string_info\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 535\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_h5ds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masstr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;66;03m# get padding\u001b[39;00m\n\u001b[1;32m    538\u001b[0m padding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding(key)\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/h5py/_hl/dataset.py:282\u001b[0m, in \u001b[0;36mAsStrView.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m--> 282\u001b[0m     bytes_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;66;03m# numpy.char.decode() seems like the obvious thing to use. But it only\u001b[39;00m\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;66;03m# accepts numpy string arrays, not object arrays of bytes (which we\u001b[39;00m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# return from HDF5 variable-length strings). And the numpy\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# implementation is not faster than doing it with a loop; in fact, by\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;66;03m# not converting the result to a numpy unicode array, the\u001b[39;00m\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;66;03m# naive way can be faster! (Comparing with numpy 1.18.4, June 2020)\u001b[39;00m\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m numpy\u001b[38;5;241m.\u001b[39misscalar(bytes_arr):\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/h5py/_hl/dataset.py:885\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, args, new_dtype)\u001b[0m\n\u001b[1;32m    883\u001b[0m mspace \u001b[38;5;241m=\u001b[39m h5s\u001b[38;5;241m.\u001b[39mcreate_simple(selection\u001b[38;5;241m.\u001b[39mmshape)\n\u001b[1;32m    884\u001b[0m fspace \u001b[38;5;241m=\u001b[39m selection\u001b[38;5;241m.\u001b[39mid\n\u001b[0;32m--> 885\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmspace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfspace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdxpl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dxpl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[38;5;66;03m# Patch up the output for NumPy\u001b[39;00m\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m ():\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5d.pyx:241\u001b[0m, in \u001b[0;36mh5py.h5d.DatasetID.read\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_proxy.pyx:142\u001b[0m, in \u001b[0;36mh5py._proxy.dset_rw\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5fd.pyx:162\u001b[0m, in \u001b[0;36mh5py.h5fd.H5FD_fileobj_read\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/fsspec/spec.py:2129\u001b[0m, in \u001b[0;36mAbstractBufferedFile.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m   2124\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"mirrors builtin file's readinto method\u001b[39;00m\n\u001b[1;32m   2125\u001b[0m \n\u001b[1;32m   2126\u001b[0m \u001b[38;5;124;03mhttps://docs.python.org/3/library/io.html#io.RawIOBase.readinto\u001b[39;00m\n\u001b[1;32m   2127\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2128\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmemoryview\u001b[39m(b)\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2129\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnbytes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2130\u001b[0m out[: \u001b[38;5;28mlen\u001b[39m(data)] \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m   2131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data)\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/fsspec/spec.py:2111\u001b[0m, in \u001b[0;36mAbstractBufferedFile.read\u001b[0;34m(self, length)\u001b[0m\n\u001b[1;32m   2108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m length \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   2109\u001b[0m     \u001b[38;5;66;03m# don't even bother calling fetch\u001b[39;00m\n\u001b[1;32m   2110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 2111\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fetch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlength\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2113\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m   2114\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m read: \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2115\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2118\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache\u001b[38;5;241m.\u001b[39m_log_stats(),\n\u001b[1;32m   2119\u001b[0m )\n\u001b[1;32m   2120\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(out)\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/fsspec/caching.py:287\u001b[0m, in \u001b[0;36mReadAheadCache._fetch\u001b[0;34m(self, start, end)\u001b[0m\n\u001b[1;32m    285\u001b[0m end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize, end \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocksize)\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_requested_bytes \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m end \u001b[38;5;241m-\u001b[39m start\n\u001b[0;32m--> 287\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# new block replaces old\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart \u001b[38;5;241m=\u001b[39m start\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache)\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/s3fs/core.py:2378\u001b[0m, in \u001b[0;36mS3File._fetch_range\u001b[0;34m(self, start, end)\u001b[0m\n\u001b[1;32m   2376\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_fetch_range\u001b[39m(\u001b[38;5;28mself\u001b[39m, start, end):\n\u001b[1;32m   2377\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2378\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_fetch_range\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2379\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2380\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbucket\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2381\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2382\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mversion_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2383\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2384\u001b[0m \u001b[43m            \u001b[49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2385\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreq_kw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreq_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2386\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2388\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m   2389\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m ex\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m errno\u001b[38;5;241m.\u001b[39mEINVAL \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpre-conditions\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m ex\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m1\u001b[39m]:\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/s3fs/core.py:2547\u001b[0m, in \u001b[0;36m_fetch_range\u001b[0;34m(fs, bucket, key, version_id, start, end, req_kw)\u001b[0m\n\u001b[1;32m   2545\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2546\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFetch: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, bucket, key, start, end)\n\u001b[0;32m-> 2547\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msync\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_inner_fetch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbucket\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mversion_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq_kw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/fsspec/asyn.py:91\u001b[0m, in \u001b[0;36msync\u001b[0;34m(loop, func, timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m asyncio\u001b[38;5;241m.\u001b[39mrun_coroutine_threadsafe(_runner(event, coro, result, timeout), loop)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;66;03m# this loops allows thread to get interrupted\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mevent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     92\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    605\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 607\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Multiple years\n",
    "\n",
    "# Set up projections\n",
    "transformer = Transformer.from_crs(source_crs, target_crs, always_xy=True)\n",
    "\n",
    "# Get iQuam file paths in directory between desired dates and find and produce matching Landsat SSTs \n",
    "for year in range(start_yr, end_yr):  \n",
    "    yrmo = []\n",
    "    start_yrmo = f\"{year}{start_mo}\"  # Start from September of the current year\n",
    "    end_yrmo = f\"{year+1}{end_mo}\"  # End in March of the next year\n",
    "\n",
    "    m0 = start_yrmo\n",
    "    \n",
    "    # Make a list of months between start and end\n",
    "    while int(m0) <= int(end_yrmo):\n",
    "        calc_dt = datetime.strptime(f'{m0[:4]}-{m0[4:]}', '%Y-%m')\n",
    "        yrmo.append(calc_dt.strftime(\"%Y%m\"))\n",
    "        m0 = (calc_dt + relativedelta(months=1)).strftime(\"%Y%m\")\n",
    "    \n",
    "    # Get file names and select only those matching dates from yrmo  \n",
    "    #iQfiles = os.listdir(iQpath)\n",
    "    s3path = 's3://pythia/landsat8/iQuam/*.nc'\n",
    "    iQfiles = s3.glob(s3path)\n",
    "    #s3.invalidate_cache()\n",
    "\n",
    "    #iQfiles = [x for x in iQfiles if x[:6] in yrmo]\n",
    "    iQfiles = [x for x in iQfiles if x[22:22+6] in yrmo]\n",
    "    iQfiles.sort(reverse=True)\n",
    "    print (f'{year}: {len(iQfiles)}')\n",
    "    \n",
    "    #os.chdir(iQpath)\n",
    "\n",
    "    # For each iquam file, pair West Antarctic Argo buoy data with Landsat data and create calibrated SSTs\n",
    "    valid = []\n",
    "    \n",
    "    for iquam_file in iQfiles:\n",
    "        print(iquam_file)\n",
    "        print('s3://'+iquam_file)\n",
    "        \n",
    "        # Open Argos data from iQuam file\n",
    "        s3file = s3.open('s3://'+iquam_file)\n",
    "        \n",
    "        df = xr.open_dataset(s3file)\n",
    "        iquam = df.to_dataframe()\n",
    "        \n",
    "        # Subset to Antarctica\n",
    "        ant = iquam[(iquam.lat<-65)&(iquam.lon>-142)&(iquam.lon<-72)&(iquam.platform_type==5.0)&(iquam.quality_level==5.0)]  # Entire West Antarctica (later)\n",
    "        # ant = iquam[(iquam.lat<-69)&(iquam.lon>-125)&(iquam.lon<-98)&(iquam.platform_type==5.0)&(iquam.quality_level==5.0)] # Amundsen Sea\n",
    "        \n",
    "        # To remove a landsat day that is coming up with a 403 error\n",
    "        if ant['year'].iloc[0] == 2020 and ant['month'].iloc[0] == 12:\n",
    "            ant = ant[ant.day != 9.0] # for 202012 because otherwise will fail\n",
    "        \n",
    "        print('')\n",
    "        print(f'{iquam_file[:6]}: {ant.shape[0]} measurements')\n",
    "    \n",
    "        for idx in tqdm(range(ant.shape[0]), desc=\"Processing\"):\n",
    "    \n",
    "            # Create search area\n",
    "            ilat = ant['lat'].iloc[idx]\n",
    "            ilon = ant['lon'].iloc[idx]\n",
    "    \n",
    "            lat_add = sut.km_to_decimal_degrees(dist, ilat, direction='latitude')\n",
    "            lon_add = sut.km_to_decimal_degrees(dist, ilat, direction='longitude')\n",
    "            bboxV = (ilon-lon_add,ilat-lat_add,ilon+lon_add,ilat+lat_add)\n",
    "    \n",
    "            # Create Landsat temporal search range in correct format\n",
    "            ihr = int(ant.hour.iloc[idx])\n",
    "            iyr = int(ant.year.iloc[idx])\n",
    "            imo = int(ant.month.iloc[idx])\n",
    "    \n",
    "            calc_dt = datetime.strptime(f'{iyr}-{imo}-{int(ant.day.iloc[idx])} {ihr}', '%Y-%m-%d %H')\n",
    "            start_dt = (calc_dt + timedelta(days=-time_add)).strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "            end_dt = (calc_dt + timedelta(days=time_add)).strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "    \n",
    "            timeRangeV = f'{start_dt}/{end_dt}'\n",
    "    \n",
    "            # Search for desired Landsat scenes\n",
    "            items = sut.search_stac(url,collection,gjson_outfile=gjson_outfile,bbox=bboxV,timeRange=timeRangeV)\n",
    "    \n",
    "            # Load the geojson file and open stac catalog\n",
    "            catalog = intake.open_stac_item_collection(items)\n",
    "            gf = gpd.read_file(gjson_outfile)\n",
    "    \n",
    "            # Exclude Landsat 9\n",
    "            catalog_list = [x for x in items if x.id[3]=='8']\n",
    "            num_scene = len(catalog_list)\n",
    "            # print(f'{num_scene} Landsat 8 items')\n",
    "    \n",
    "            # If any matching landsat scenes are found create calibrated SSTs for them\n",
    "            if num_scene>0:\n",
    "    \n",
    "                # Reproject to determine bounding box in espg 3031\n",
    "                sbox,checkbox = sut.lsat_reproj(source_crs,target_crs,(bboxV[0],bboxV[1],bboxV[2],bboxV[3]))\n",
    "    \n",
    "                # Create polygon for later cropping\n",
    "                polygon = Polygon([(sbox[0][0],sbox[0][1]),(sbox[3][0],sbox[3][1]),(sbox[2][0],sbox[2][1]),(sbox[1][0],sbox[1][1])])\n",
    "                \n",
    "                # Create min/max boundaries for trimming image before crop_xarray to cut down on processing times\n",
    "                minx, miny, maxx, maxy = polygon.bounds\n",
    "                polarx = [minx, maxx]\n",
    "                polary = [miny, maxy]\n",
    "    \n",
    "                # Create calibrated SSTs for each matching landsat scene\n",
    "                for sceneid in catalog_list:\n",
    "                    print(sceneid.id)\n",
    "    \n",
    "                    scene = catalog[sceneid.id]\n",
    "                    timestr = scene.metadata['datetime'].strftime('%H%M%S')\n",
    "    \n",
    "                    outFile = f'{SSTpath}/{sceneid.id}_{timestr}_Cel.tif'\n",
    "    \n",
    "                    if os.path.isfile(outFile):\n",
    "                        print (f'{sceneid.id} - atm corr exists')\n",
    "                        ls_scene = xr.open_dataset(outFile,chunks=dict(x=512, y=512),engine='rasterio')\n",
    "                        ls_scene = ls_scene.rio.write_crs(\"epsg:3031\", inplace=True)\n",
    "    \n",
    "                        # Subset all scenes and check for right dimensions because y order changes sometimes\n",
    "                        ls_sub = sut.subset_img(ls_scene['band_data'].sel(band=1),polarx,polary) # subset so easier to work with\n",
    "    \n",
    "                    else:\n",
    "                        # try:\n",
    "                        # Open all desired bands for one scene\n",
    "                        ls_scene = sut.landsat_to_xarray(sceneid,catalog)\n",
    "                        ls_scene = ls_scene.rio.write_crs(\"epsg:3031\", inplace=True)\n",
    "        \n",
    "                        # Create classification mask\n",
    "                        ls_scene = sut.create_masks(ls_scene, cloud_mask=True, ice_mask=True, ocean_mask=True)\n",
    "\n",
    "                        # Check for any open ocean pixels - go to next image if none - ??? s\n",
    "                        mask = np.ones(ls_scene.shape[1:])\n",
    "                        mask[ls_scene.mask!=3] = np.nan\n",
    "                        ls_thermal = ls_scene.sel(band='lwir11').compute()\n",
    "\n",
    "                        # Check in bounding box or for entire Landsat image depending on whether doing calibration runs or not\n",
    "                        ls_box = sut.subset_img(ls_thermal*mask,polarx,polary)\n",
    "\n",
    "                        if ((ls_box).notnull()).sum().values==0:\n",
    "                            print (f'{sceneid.id} has no SSTs')\n",
    "                            try:\n",
    "                                del ls_scene, scene, mask, ls_thermal, ls_box\n",
    "                            except:\n",
    "                                pass\n",
    "                            gc.collect()\n",
    "                            continue\n",
    "\n",
    "                        # Use band ratios for RF cloud pixel classification\n",
    "                        ####\n",
    "\n",
    "                        # Atmospheric correction using MODIS\n",
    "                        # Acquire and align MODIS water vapor (MOD/MYD07) to Landsat\n",
    "                        mod07,modfilenm = sut.open_MODIS(ls_scene,scene,modout_path)\n",
    "                        print(\"1\")\n",
    "\n",
    "                        # Create water vapor files aligned and subsampled to Landsat\n",
    "                        spacing = [300,-300] # 300m sampling of MODIS data so that upsampling is easy and because 30m takes far too long\n",
    "                        WV_xr = sut.get_wv(ls_scene,mod07,spacing,WV,scene,interp=interp)\n",
    "                        print(\"2\")\n",
    "                        # Create SST by masking and using water vapor to apply atmospheric correction\n",
    "                        SST = sut.apply_retrieval(ls_thermal,scene,mask,WV_xr,atmcor,simT_transformer,simTOA_transformer,simWV_transformer)\n",
    "\n",
    "                        # Record MODIS water vapor image used in atmospheric correction, will find this info save under band_data in\n",
    "                        # data variables in COG (click on white paper info button in xarray readout)\n",
    "                        SST.attrs['MODIS_WV'] = modfilenm\n",
    "\n",
    "                        # Save to a cloud-optimized Geotiff\n",
    "                        SST.rio.to_raster(raster_path=outFile, driver=\"COG\")\n",
    "                        print (f'Mean SST: {np.nanmean(SST)}')\n",
    "\n",
    "                        # Subset all scenes and check for right dimensions because y order changes sometimes\n",
    "                        ls_sub = sut.subset_img(SST,polarx,polary) # subset so easier to work with \n",
    "\n",
    "                        try:\n",
    "                            del mask, ls_thermal, mod07, WV_xr, SST\n",
    "                        except:\n",
    "                            pass\n",
    "    \n",
    "                        # except Exception as e:\n",
    "                        #     print (sceneid.id, e)\n",
    "                        #     lsat = np.nan\n",
    "                        #     lstd = np.nan\n",
    "    \n",
    "                    # Crop data to exact bounding box\n",
    "                    ls_sub = sut.crop_xarray_dataarray_with_polygon(ls_sub, polygon) \n",
    "    \n",
    "                    # Calibrate using MODIS\n",
    "                    ls_sub = ls_sub * calib_m + calib_b\n",
    "    \n",
    "                    # Remove all pixels that are too cold\n",
    "                    ls_sub = ls_sub.where(ls_sub>=lthresh,np.nan)\n",
    "    \n",
    "                    lsat = np.around(np.nanmean(ls_sub),2)\n",
    "                    lstd = np.around(np.nanstd(ls_sub),2)\n",
    "    \n",
    "                    # Convert Argo lat/lon to Landsat's EPSG:3031\n",
    "                    argo_px, argo_py = transformer.transform(ilon, ilat)\n",
    "    \n",
    "                    # 1) Find the nearest center pixel\n",
    "                    center_val = ls_sub.sel(x=argo_px, y=argo_py, method=\"nearest\")\n",
    "                    \n",
    "                    # Extract the x/y coordinate values as plain floats\n",
    "                    center_x = center_val.x.item()\n",
    "                    center_y = center_val.y.item()\n",
    "                    \n",
    "                    # 2) Get integer indices from the coordinate indexes\n",
    "                    #    This uses ls_sub.get_index('dim_name') -> pandas.Index -> get_indexer(...)\n",
    "                    center_x_idx = ls_sub.get_index(\"x\").get_indexer([center_x])[0]\n",
    "                    center_y_idx = ls_sub.get_index(\"y\").get_indexer([center_y])[0]\n",
    "                    \n",
    "                    # 3) Gather offsets for the 3x3 neighborhood\n",
    "                    offsets = [\n",
    "                        ( 0,  0, \"center\"),\n",
    "                        ( 0,  1, \"N\"),\n",
    "                        ( 0, -1, \"S\"),\n",
    "                        ( 1,  0, \"E\"),\n",
    "                        (-1,  0, \"W\"),\n",
    "                        ( 1,  1, \"NE\"),\n",
    "                        ( 1, -1, \"SE\"),\n",
    "                        (-1,  1, \"NW\"),\n",
    "                        (-1, -1, \"SW\"),\n",
    "                    ]\n",
    "                    \n",
    "                    neighbors = {}\n",
    "                    for dx, dy, name in offsets:\n",
    "                        nx = center_x_idx + dx\n",
    "                        ny = center_y_idx + dy\n",
    "                        # Ensure we're within the array bounds\n",
    "                        if (0 <= nx < ls_sub.sizes['x']) and (0 <= ny < ls_sub.sizes['y']):\n",
    "                            # xarray dimension order is typically (y, x), so use isel(y=ny, x=nx):\n",
    "                            neighbors[name] = ls_sub.isel(y=ny, x=nx).values.item()\n",
    "                        else:\n",
    "                            neighbors[name] = np.nan\n",
    "                    \n",
    "                    # 4) Record coincident data from Landsat and Argo float\n",
    "                    argo_temp = np.around((ant.sst.iloc[idx] - 273.15),2)  # convert to Celsius\n",
    "                    times = pd.to_datetime(calc_dt, format='%Y%m%d%H')  # standardize time\n",
    "                    \n",
    "                    valid.append([\n",
    "                        times,\n",
    "                        sceneid.id,\n",
    "                        lsat,\n",
    "                        lstd,\n",
    "                        neighbors['center'],\n",
    "                        neighbors['N'],\n",
    "                        neighbors['S'],\n",
    "                        neighbors['E'],\n",
    "                        neighbors['W'],\n",
    "                        neighbors['NE'],\n",
    "                        neighbors['SE'],\n",
    "                        neighbors['NW'],\n",
    "                        neighbors['SW'],\n",
    "                        ls_sub.max().values.item(),\n",
    "                        ls_sub.min().values.item(),\n",
    "                        ant.iloc[idx].name,  # Argo ID or whichever label you prefer\n",
    "                        argo_temp,\n",
    "                        ilat,\n",
    "                        ilon\n",
    "                    ])\n",
    "                    print (f'Argo temp: {argo_temp}, Landsat 8 mean: {lsat}+/-{lstd}')\n",
    "    \n",
    "                    try:\n",
    "                        del ls_scene, scene, ls_thermal, ls_box, mod07, WV_xr, SST, ls_sub, neighbors\n",
    "                    except:\n",
    "                        pass\n",
    "    \n",
    "                    gc.collect()\n",
    "\n",
    "    # Put data into DataFrame and save    \n",
    "    lsat_mod_df = pd.DataFrame(valid,columns=headers)\n",
    "    out_df = f'Landsat_validation_{start_yrmo}_{end_yrmo}_{dist}.csv'\n",
    "    lsat_mod_df.to_csv(out_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13eacc2c-ab93-4df3-8afa-418f77041d05",
   "metadata": {},
   "source": [
    "### Landsat-iQUAM validation assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b61cb9-8146-46d6-ace8-112e67fc7eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nothing has been tested below this point after accessing iQuam data from Jetstream\n",
    "# KM 8/8/25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c9d945-424a-4749-98e3-49b3fc216ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in processed matchups\n",
    "\n",
    "# 20250107 is threshold=-1.9, 20250114 is thresh=-1.94, 20250205 is thresh=-1.9\n",
    "out_df = lsatpath / 'Landsat_validation_202009_202103_1.0.csv'\n",
    "valids = pd.read_csv(out_df)\n",
    "\n",
    "valids = valids.set_index('DateTime')\n",
    "valids = valids.sort_index()\n",
    "valids['Argo_id'] = valids['Argo_id'].astype(int)\n",
    "\n",
    "# Remove the non-numeric column for calculating daily means\n",
    "numeric_valids = valids.select_dtypes(include=[np.number])\n",
    "validmn = numeric_valids.groupby(numeric_valids['Argo_id']).mean()\n",
    "valids = valids.reset_index()\n",
    "\n",
    "# Group by the date component of the datetime and calculate the difference\n",
    "valids['temp_dif'] = valids.groupby(valids['Argo_id'])[f'L8_SST'].diff()\n",
    "validmn['temp_dif'] = valids.groupby(valids['Argo_id'])['temp_dif'].first()\n",
    "valids = valids.set_index('DateTime')\n",
    "\n",
    "validmn['std'] = valids.groupby([valids['Argo_id']])[f'L8_SST'].std()\n",
    "validmn['xaxis'] = pd.to_datetime(validmn.index).dayofyear\n",
    "validmn['xaxis'][validmn['xaxis']<(365/2)] = validmn['xaxis'] + 365\n",
    "\n",
    "valids['xaxis'] = pd.to_datetime(valids.index).dayofyear\n",
    "valids['xaxis'][valids['xaxis']<(365/2)] = valids['xaxis'] + 365\n",
    "\n",
    "validmn = validmn.dropna(subset=['L8_SST'])\n",
    "\n",
    "valids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fe69e4-a23e-47d5-9108-e5dbc821ae5b",
   "metadata": {},
   "source": [
    "### Visualize all validation matchups for manual QC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607b42a7-4f5f-422d-9748-69df0ead7766",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setup and authenticate \n",
    "from dask.distributed import Client\n",
    "import logging\n",
    "client = Client(processes=True, n_workers=4, \n",
    "                threads_per_worker=1,\n",
    "                silence_logs=logging.ERROR)\n",
    "client.run(lambda: os.environ[\"AWS_REQUEST_PAYER\"] == \"requester\" )\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2977e506-8e86-4263-925f-c3d4c6fcefeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de6c02a-740c-4025-9119-2e95d36fc29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(lsatpath)\n",
    "lsatfiles = os.listdir(lsatpath)\n",
    "lsatfiles = [i for i in lsatfiles if i[0]=='L']\n",
    "\n",
    "# Remove all files from the list with repeats and Landsat mean = nan\n",
    "remove = [\n",
    "'LC08_L1GT_029109_20160922_20200906_02_T2', \n",
    "'LC08_L1GT_022110_20181029_20201016_02_T2', \n",
    "'LC08_L1GT_005110_20181209_20201016_02_T2',\n",
    "'LC08_L1GT_228108_20200123_20201016_02_T2',\n",
    "'LC08_L1GT_001108_20200929_20201006_02_T2',\n",
    "'LC08_L1GT_002109_20201022_20201105_02_T2',\n",
    "'LC08_L1GT_233108_20201109_20210317_02_T2',\n",
    "'LC08_L1GT_233109_20201109_20210317_02_T2',\n",
    "'LC08_L1GT_001109_20201202_20210312_02_T2',\n",
    "'LC08_L1GT_027112_20220128_20220204_02_T2',\n",
    "'LC08_L1GT_002109_20210331_20210408_02_T2'\n",
    "]\n",
    "\n",
    "lsatfiles = [i for i in lsatfiles if i[:-15] not in remove]\n",
    "if len(lsatfiles)!=12:\n",
    "    print('Wrong number of Landsat scenes!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3725c3-2885-4faa-84f4-1f6768de41d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of columns/rows for subplots\n",
    "n_cols = 3\n",
    "n_rows = 4\n",
    "\n",
    "# Create one figure and a 4x7 grid of subplots\n",
    "# The figsize is 10 across; adjust height as needed for clarity\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=n_rows,\n",
    "    ncols=n_cols,\n",
    "    figsize=(11, 8.5),   \n",
    "    subplot_kw={'projection': ccrs.PlateCarree()}\n",
    ")\n",
    "\n",
    "# Optional: if you want them *really* close, you can fine-tune spacing:\n",
    "plt.subplots_adjust(wspace=0.05, hspace=0.03)\n",
    "# plt.subplots_adjust(bottom=0.15)\n",
    "\n",
    "# Counter for subplot index, and a handle to store the last \"imshow\" (for colorbar)\n",
    "i = 0\n",
    "im_obj = None\n",
    "\n",
    "for lsatfile in lsatfiles:\n",
    "    lsID = lsatfile\n",
    "    print(lsID)\n",
    "    \n",
    "    mrow = valids[valids['L8_filename'].str.contains(lsatfile[:-15])]\n",
    "    \n",
    "    for idx, row in mrow.iterrows():\n",
    "        # Check if L8_SST is NaN\n",
    "        if pd.isnull(row['L8_SST']):\n",
    "            # If it is NaN, skip this iteration and do not plot\n",
    "            continue\n",
    "        \n",
    "        # --- Prepare data and coordinates ---\n",
    "        ilat = row['A_lat']\n",
    "        ilon = row['A_lon']\n",
    "        \n",
    "        lat_add = km_to_decimal_degrees(dist, ilat, direction='latitude')\n",
    "        lon_add = km_to_decimal_degrees(dist, ilat, direction='longitude')\n",
    "        xmin, ymin, xmax, ymax = (ilon - lon_add, ilat - lat_add, \n",
    "                                  ilon + lon_add, ilat + lat_add)\n",
    "        \n",
    "        # Load the Landsat file\n",
    "        ds = xr.open_dataset(lsatfile, chunks=dict(x=512, y=512), engine='rasterio')\n",
    "        ls_scene = ds['band_data'].sel(band=1).rio.write_crs(\"epsg:3031\", inplace=True)\n",
    "        \n",
    "        # Assign time coordinate\n",
    "        times = pd.to_datetime(lsatfile[17:25] + lsatfile[41:47], format='%Y%m%d%H%M%S')\n",
    "        ls_scene = ls_scene.assign_coords(time=times, ID=lsatfile[:-8])\n",
    "        \n",
    "        # Reproject to EPSG:4326\n",
    "        ls_scene = ls_scene.rio.reproject(\"EPSG:4326\")\n",
    "\n",
    "        # Calibrate using MODIS\n",
    "        ls_scene = ls_scene * calib_m + calib_b\n",
    "\n",
    "        # Remove all pixels that are too cold\n",
    "        ls_scene = ls_scene.where(ls_scene>=lthresh,np.nan)\n",
    "        \n",
    "        # Select the subplot axis\n",
    "        ax = axes[i // n_cols, i % n_cols]\n",
    "        \n",
    "        # Plot on that axis, without a colorbar\n",
    "        # Store the \"imshow\" result in im_obj so we can build one colorbar later\n",
    "        im_obj = ls_scene.plot.imshow(\n",
    "            x='x', y='y',\n",
    "            vmin=-2.0, vmax=1.5,\n",
    "            ax=ax,\n",
    "            transform=ccrs.PlateCarree(),\n",
    "            origin='upper',\n",
    "            add_colorbar=False  # <- No individual colorbar\n",
    "        )\n",
    "        \n",
    "        # Remove titles completely (xarray may add one by default)\n",
    "        ax.set_title('')\n",
    "        \n",
    "        # Set extent\n",
    "        ax.set_extent([ilon - 0.4, ilon + 0.4, ilat - 0.2, ilat + 0.2], crs=ccrs.PlateCarree())\n",
    "        \n",
    "        # Argo observation\n",
    "        ax.scatter([ilon], [ilat], c='r', s=3, transform=ccrs.PlateCarree(), label='Argo location')\n",
    "        \n",
    "        # Draw bounding box\n",
    "        polygon_show = Pgon([(xmin, ymin), (xmin, ymax), \n",
    "                             (xmax, ymax), (xmax, ymin)],\n",
    "                            closed=True, edgecolor='red', facecolor='none')\n",
    "        ax.add_patch(polygon_show)\n",
    "        \n",
    "        # Text label\n",
    "        text_str = (\n",
    "            f\"Argo ID: {row['Argo_id']}\\n\"\n",
    "            f\"Argo Temp: {np.around(row['Argo_SST'], 2)}\\n\"\n",
    "            f\"Landsat SST: {np.around(row['L8_SST'], 2)}\"\n",
    "        )\n",
    "        ax.text(\n",
    "            0.02, 0.95,\n",
    "            text_str,\n",
    "            transform=ax.transAxes,\n",
    "            fontsize=10,\n",
    "            va='top',\n",
    "            ha='left'\n",
    "        )\n",
    "\n",
    "        gc.collect()\n",
    "        \n",
    "        # Move to next subplot index\n",
    "        i += 1\n",
    "        \n",
    "\n",
    "# --- Add a single colorbar for the entire figure ---\n",
    "# We use the last \"imshow\" (im_obj) and attach to all subplot axes\n",
    "cbar_ax = fig.add_axes([0.25, 0.05, 0.5, 0.03])\n",
    "\n",
    "cbar = fig.colorbar(\n",
    "    im_obj, \n",
    "    ax=axes.ravel().tolist(),  # or just ax=axes if axes is 2D\n",
    "    cax=cbar_ax,\n",
    "    orientation='horizontal',  # 'vertical' or 'horizontal'\n",
    "    fraction=0.025,            # how long the colorbar is relative to axes\n",
    "    pad=0.05                   # space between colorbar and subplots\n",
    ")\n",
    "cbar.set_label(\"Temperature [C]\", fontsize=12)\n",
    "\n",
    "# plt.tight_layout()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6366390a-5c68-4101-b5bb-306a538a048d",
   "metadata": {},
   "source": [
    "# Use matchups to compare modeled and observed data\n",
    "Here, we are going to use a linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52881003-d49a-40f8-bb31-3b1379960a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove low quality validation data from the valids data\n",
    "rm_ids = [1460597, 1614080, 2143782, 2016236, 1790883]\n",
    "rm = validmn[validmn.index.isin(rm_ids)]\n",
    "validmn = validmn.drop(index=rm_ids, errors='ignore')\n",
    "valids = valids[~valids['Argo_id'].isin(rm_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9832c9-be57-4022-90f5-ca22496eca7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Orthoganal Regression \n",
    "data = validmn\n",
    "\n",
    "# Original data\n",
    "x_original = np.array(data['Argo_SST'])\n",
    "y_original = np.array(data['L8_SST'])\n",
    "\n",
    "# Define a linear function for the model\n",
    "def linear_model(p, x):\n",
    "    return p[0] * x + p[1]\n",
    "\n",
    "# Create a Model\n",
    "linear = Model(linear_model)\n",
    "\n",
    "# Create a RealData object using your DataFrame\n",
    "data = RealData(x_original, y_original)\n",
    "\n",
    "# Set up ODR with the model and data\n",
    "odr = ODR(data, linear, beta0=[1., 0.])\n",
    "\n",
    "# Run the regression\n",
    "out = odr.run()\n",
    "\n",
    "# Use the output\n",
    "beta = out.beta\n",
    "beta_err = out.sd_beta\n",
    "\n",
    "# Print the summary\n",
    "out.pprint()\n",
    "\n",
    "# Predicting values using the ODR model\n",
    "y_pred = linear_model(beta, x_original)\n",
    "\n",
    "# Get R2\n",
    "# Calculate Total Sum of Squares (SST)\n",
    "y_mean = np.mean(y_original)\n",
    "SST = np.sum((y_original - y_mean)**2)\n",
    "\n",
    "# Calculate Residual Sum of Squares (SSR)\n",
    "SSR = np.sum((y_original - y_pred)**2)\n",
    "\n",
    "# Compute RMSE\n",
    "rmse = np.sqrt(((y_original - y_pred) ** 2).mean())\n",
    "\n",
    "# Calculate R^2\n",
    "R2 = 1 - (SSR / SST)\n",
    "print(\"R^2:\", np.around(R2,2))\n",
    "print(f\"RMSE: {np.around(rmse,2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe231c0-2207-4c2a-8f9f-c1afa7ecd923",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "beta_mdn = [beta[0]-beta_err[0]*1.96,beta[1]-beta_err[1]*1.96]\n",
    "beta_mup = [beta[0]+beta_err[0]*1.96,beta[1]-beta_err[1]*1.96]\n",
    "beta_bdn = [beta[0]-beta_err[0]*1.96,beta[1]+beta_err[1]*1.96]\n",
    "beta_bup = [beta[0]+beta_err[0]*1.96,beta[1]+beta_err[1]*1.96]\n",
    "print(f'At 95% confidence interval: {np.around(beta[0],2)}+/-{np.around(beta_err[0]*1.96,2)}, {np.around(beta[1],2)}+/-{np.around(beta_err[1]*1.96,2)}, n={y_pred.shape[0]}')\n",
    "xfill = np.array([-4.3,0.9])\n",
    "\n",
    "\n",
    "# Plot data points and 1:1 line\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "ax.tick_params(labelsize=14)\n",
    "\n",
    "xi = np.arange(-7.0,5.0,1.0)\n",
    "\n",
    "# lower_err = abs(data['L8_SST'] - data['L8_SST_min'])  # distance to lower bound\n",
    "# upper_err = abs(data['L8_SST_max'] - data['L8_SST'])  # distance to upper bound\n",
    "\n",
    "ax.scatter(x_original,y_original,color='k',linewidth=0,s=35,label='High quality')\n",
    "ax.errorbar(x_original,y_original,yerr=validmn['std'],color='k',fmt='o',ecolor='k',elinewidth=1,capthick=1,marker='o',ms=3,capsize=5,label='_no legend_')\n",
    "# ax.errorbar(data['Argo_SST'],data['L8_SST'],yerr=[lower_err,upper_err],color='k',fmt='o',ecolor='k',elinewidth=1,capthick=1,marker='o',ms=3,capsize=5,label='_no legend_')\n",
    "# ax.scatter(data['Argo_SST'],data['center'],color='r',linewidth=0,s=25,label='_no label_')\n",
    "ax.scatter(rm['Argo_SST'],rm['L8_SST'],color='0.7', s=35, label='Removed',zorder=2)\n",
    "ax.plot(xi,xi,color='k',linewidth=2, label='1:1')\n",
    "ax.plot(x_original, y_pred, color='k', ls=':', label='Validation ODR')\n",
    "ax.fill_between(xfill, linear_model(beta_bdn, xfill), linear_model(beta_mup, xfill),alpha=0.1, facecolor='0.3')\n",
    "# a1.plot(xi,xi*lreg.coef_[0]+lreg.intercept_[0],color='r',linewidth=2,label='RANSAC regression')\n",
    "# a1.scatter(xC,yLC,color='r',linewidth=0,s=35,label='_no label_')\n",
    "# a1.plot(xi,xi*resultC.params.L8_SST+resultC.params.Intercept,color='k',linewidth=2,label='OLS regression')\n",
    "# a1.text(-2.1,-2.85,f'y={np.around(resultC.params.L8_SST,2)}x+{np.around(resultC.params.Intercept,2)}   p={p_val}',fontsize=12)\n",
    "ax.text(-1.1,-1.5,f'y={np.around(beta[0],2)}x+{np.around(beta[1],2)}   $r^2$={np.around(R2,2)}',fontsize=14)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.set_ylim([-1.95,1.5])\n",
    "ax.set_xlim([-1.95,1.5])\n",
    "ax.set_xlabel('Argo Temperature [C]',fontsize=16)\n",
    "ax.set_ylabel('Landsat SST [C]',fontsize=16)\n",
    "ax.legend(markerscale=1,fontsize=12,loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig('/Users/tsnow03/GoogleDrive/User/Docs/PhD_Project/Manuscripts/AmundsenCC_Manuscript/Figures/LMCalibration.jpg', format='jpg', dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8015d2-fd8f-4a40-8b3c-0c06d03e6bb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
